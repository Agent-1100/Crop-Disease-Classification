{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom PIL import Image\nfrom itertools import cycle\nfrom skimage import color\nimport math\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom scipy.signal import wiener\n\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, roc_curve, auc,\n    precision_recall_curve, matthews_corrcoef\n)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.applications import (\n    VGG16, VGG19, ResNet50, InceptionV3, Xception, MobileNetV2, \n    DenseNet121, EfficientNetB0, InceptionResNetV2\n)\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.metrics import Precision\n\nseed = 42  # Or any integer you prefer\n\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 1. CONFIGURATION AND SETUP ---","metadata":{}},{"cell_type":"markdown","source":"# --- Dataset and Output Paths ---","metadata":{}},{"cell_type":"code","source":"SOURCE_DIR = '/kaggle/input/plantvillage-dataset'\nBASE_DIR = '/kaggle/working/agps_dataset'\nOUTPUT_DIR = '/kaggle/working/model_outputs'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Model and Training Parameters ---","metadata":{}},{"cell_type":"code","source":"IMG_HEIGHT = 224\nIMG_WIDTH = 224\nBATCH_SIZE = 32\nEPOCHS = 25\nEARLY_STOPPING_PATIENCE = 10\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CLASS_MAP = {\n    \"Apple___Apple_scab\": \"Apple_Scab\",\n    \"Apple___Black_rot\": \"Apple_Black_Rot\",\n    \"Apple___healthy\": \"Apple_Healthy\",\n    \"Grape___Black_rot\": \"Grape_Black_rot\",\n    \"Grape___Esca_(Black_Measles)\": \"Grape_Esca\",\n    \"Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\": \"Grape_Leaf_Blight\",\n    \"Grape___healthy\": \"Grape_Healthy\",\n    \"Peach___Bacterial_spot\": \"Peach_Bacterial_spot\",\n    \"Peach___healthy\": \"Peach_Healthy\",\n    \"Strawberry___Leaf_scorch\": \"Strawberry_Leaf_scorch\",\n    \"Strawberry___healthy\": \"Strawberry_Healthy\",\n}\n# # NEW: This will be set dynamically based on the folders found.\nNUM_CLASSES = 0 \nIMAGES_PER_CLASS = 450","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Preprocessing Toggles ---","metadata":{}},{"cell_type":"code","source":"Selector_COLOR_SEG = True\nif(Selector_COLOR_SEG):\n    Data_Selector='color'\nelse:\n    Data_Selector='segmented'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"APPLY_SEGMENTATION = False\nAPPLY_CT_WINDOW = False\nAPPLY_CLAHE = False\nAPPLY_GAUSSIAN_BLUR = False \nAPPLY_MEDIAN_FILTER = False\nAPPLY_WIENER_FILTER = False\nAPPLY_HISTOGRAM_EQUALIZATION = False\nAPPLY_LAPLACIAN_FILTER = False\nAPPLY_AVERAGE_FILTER = False\nAPPLY_SOBEL_FILTER = False\nAPPLY_CANNY_FILTER = False\nAPPLY_STAIN_NORMALIZATION = False\n\n# --- Advanced Pre-processing & Augmentation Flags ---\nAPPLY_HAIR_REMOVAL = False\nAPPLY_BILATERAL_FILTER = False\nAPPLY_RANDOM_ERASING = False\nAPPLY_GAN_AUGMENTATION = False # Master switch for GAN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- GAN-Specific Configuration (MODIFIED) ---\nif APPLY_GAN_AUGMENTATION:\n    CLASSES_TO_AUGMENT = TARGET_CLASSES\n    NUM_IMAGES_TO_GENERATE = 100\n    GAN_IMG_SIZE = 224  # MODIFIED: Changed to 224 to match the classifier\n    LATENT_DIM = 100\n    GAN_EPOCHS = 75 # Increased epochs slightly for the more complex generation task","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_directories():\n    \"\"\"\n    Collects a limited number of images from 'train' and 'val' source subfolders,\n    and then creates a new, cleanly named dataset with a fresh train/val/test split.\n    \"\"\"\n    # 1. Clean up old directory and create the new structure.\n    if os.path.exists(BASE_DIR):\n        shutil.rmtree(BASE_DIR)\n\n    train_dir = os.path.join(BASE_DIR, 'train')\n    val_dir = os.path.join(BASE_DIR, 'val')\n    test_dir = os.path.join(BASE_DIR, 'test')\n    os.makedirs(train_dir); os.makedirs(val_dir); os.makedirs(test_dir)\n\n    print(f\"New dataset will be created at: {BASE_DIR}\")\n\n    # 2. Iterate through your class mapping.\n    for source_folder, dest_folder in CLASS_MAP.items():\n        print(f\"\\nProcessing: '{source_folder}' -> '{dest_folder}'\")\n\n        all_files = []\n\n        # 3. Collect all files from BOTH the train and val directories.\n        for subfolder in [Data_Selector]:\n            src_path = os.path.join(SOURCE_DIR, subfolder, source_folder)\n\n            if os.path.exists(src_path):\n                files_found = [os.path.join(src_path, f) for f in os.listdir(src_path)\n                               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n                all_files.extend(files_found)\n                print(f\"  - Found {len(files_found)} images in '{subfolder}' directory.\")\n            else:\n                print(f\"  - WARNING: Directory not found, skipping: {src_path}\")\n\n        if not all_files:\n            print(f\"  - No images found for this class. Skipping.\")\n            continue\n            \n        # 4. Shuffle all found files randomly.\n        np.random.shuffle(all_files)\n        \n        # 5. Limit the number of images per class based on the variable.\n        if len(all_files) > IMAGES_PER_CLASS:\n            print(f\"  - Total found: {len(all_files)}. Limiting to {IMAGES_PER_CLASS} images.\")\n            all_files = all_files[:IMAGES_PER_CLASS]\n        else:\n            print(f\"  - Total found: {len(all_files)}. Using all available images.\")\n\n        # 6. Create the new destination folders (e.g., 'train/Dyskeratosis').\n        for d in [train_dir, val_dir, test_dir]:\n            os.makedirs(os.path.join(d, dest_folder), exist_ok=True)\n            \n        # 7. Split the (potentially limited) list of files.\n        train_end = int(len(all_files) * 0.7)\n        val_end = train_end + int(len(all_files) * 0.15)\n        \n        train_files = all_files[:train_end]\n        val_files = all_files[train_end:val_end]\n        test_files = all_files[val_end:]\n        \n        # 8. Copy the files into their new homes.\n        for f_path in train_files: shutil.copy(f_path, os.path.join(train_dir, dest_folder, os.path.basename(f_path)))\n        for f_path in val_files: shutil.copy(f_path, os.path.join(val_dir, dest_folder, os.path.basename(f_path)))\n        for f_path in test_files: shutil.copy(f_path, os.path.join(test_dir, dest_folder, os.path.basename(f_path)))\n            \n    print(\"\\nDataset setup and file copying complete!\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 2. PREPROCESSING FUNCTIONS ---","metadata":{}},{"cell_type":"code","source":"def build_generator(latent_dim):\n    \"\"\"\n    Builds the DCGAN Generator model, re-architected to output 224x224 images.\n    \"\"\"\n    model = keras.Sequential([\n        # Start with a 7x7 spatial size\n        layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(latent_dim,)),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        layers.Reshape((7, 7, 256)),\n\n        # Upsampling block 1: 7x7 -> 14x14\n        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # Upsampling block 2: 14x14 -> 28x28\n        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # Upsampling block 3: 28x28 -> 56x56\n        layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # Upsampling block 4: 56x56 -> 112x112\n        layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        \n        # Upsampling block 5: 112x112 -> 224x224\n        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'),\n        # Final output shape: (224, 224, 3)\n    ], name=\"generator\")\n    return model\n\ndef build_discriminator(input_shape):\n    \"\"\"Builds the DCGAN Discriminator model.\"\"\"\n    model = keras.Sequential([\n        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=input_shape),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        \n        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        \n        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        \n        layers.Flatten(),\n        layers.Dense(1) # Logit output\n    ], name=\"discriminator\")\n    return model\n\ndef train_gan_and_generate_images(class_name, num_to_generate, base_train_dir):\n    \"\"\"Trains a DCGAN on a specific class and saves generated images to the training folder.\"\"\"\n    print(f\"\\n--- Starting GAN Augmentation for class: {class_name} ---\")\n    target_dir = os.path.join(base_train_dir, class_name)\n    if not os.path.exists(target_dir):\n        print(f\"ERROR: Cannot find source directory for GAN training: {target_dir}\")\n        return\n\n    # 1. Load Data, resizing to the new GAN_IMG_SIZE\n    real_images = []\n    for filename in os.listdir(target_dir):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img = load_img(os.path.join(target_dir, filename), target_size=(GAN_IMG_SIZE, GAN_IMG_SIZE))\n            real_images.append(img_to_array(img))\n    \n    if len(real_images) < BATCH_SIZE:\n        print(f\"WARNING: Not enough images ({len(real_images)}) in {target_dir} to train GAN with batch size {BATCH_SIZE}. Skipping.\")\n        return\n        \n    real_images = (np.array(real_images) - 127.5) / 127.5\n    train_dataset = tf.data.Dataset.from_tensor_slices(real_images).shuffle(len(real_images)).batch(BATCH_SIZE, drop_remainder=True)\n\n    # 2. Build Models with the correct input shape\n    generator = build_generator(LATENT_DIM)\n    discriminator = build_discriminator((GAN_IMG_SIZE, GAN_IMG_SIZE, 3))\n    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n    def discriminator_loss(real_output, fake_output):\n        return cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n\n    def generator_loss(fake_output):\n        return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n    gen_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)\n    disc_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)\n\n    @tf.function\n    def train_step(images):\n        noise = tf.random.normal([images.shape[0], LATENT_DIM])\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = generator(noise, training=True)\n            real_output = discriminator(images, training=True)\n            fake_output = discriminator(generated_images, training=True)\n            gen_loss = generator_loss(fake_output)\n            disc_loss = discriminator_loss(real_output, fake_output)\n        \n        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n        grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n        disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n\n    # 3. Training Loop\n    print(f\"Training GAN for {GAN_EPOCHS} epochs on {GAN_IMG_SIZE}x{GAN_IMG_SIZE} images...\")\n    for epoch in range(GAN_EPOCHS):\n        for image_batch in train_dataset:\n            train_step(image_batch)\n        if (epoch + 1) % 10 == 0:\n            print(f\"  - GAN Epoch {epoch + 1}/{GAN_EPOCHS} completed.\")\n\n    # 4. Generate and Save Images\n    print(\"Generating synthetic images...\")\n    noise = tf.random.normal([num_to_generate, LATENT_DIM])\n    generated_images = generator(noise, training=False)\n    generated_images = (generated_images * 127.5 + 127.5).numpy().astype(np.uint8)\n    \n    for i, img_array in enumerate(generated_images):\n        img = array_to_img(img_array)\n        # REMOVED: No longer need to resize, as the GAN generates at the correct size\n        img.save(os.path.join(target_dir, f'synthetic_gan_{i+1}.png'))\n        \n    print(f\"Successfully added {num_to_generate} synthetic images to '{target_dir}'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_ct_window(image):\n    \"\"\"\n    Applies a soft tissue window to a CT scan.\n    This is a critical step to normalize the Hounsfield Units (HU)\n    to a range that highlights soft tissues like the kidney.\n\n    Args:\n        image (numpy.ndarray): Input CT image. Assumes it's a grayscale image.\n                               If it's RGB, it will be converted to grayscale.\n\n    Returns:\n        numpy.ndarray: Windowed image in uint8 format.\n    \"\"\"\n    # These values are standard for a soft tissue window.\n    window_level = 50  # Center of the HU range\n    window_width = 400 # Width of the HU range\n\n    # If the image is RGB (like the sample), convert to grayscale first.\n    # Medical images like DICOM are often single-channel.\n    if len(image.shape) == 3 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Formula to apply windowing\n    min_hu = window_level - window_width // 2\n    max_hu = window_level + window_width // 2\n    \n    # Clip the image to the desired HU range\n    img_windowed = np.clip(image, min_hu, max_hu)\n    \n    # Normalize to 0-255 for visualization and further processing\n    img_normalized = cv2.normalize(img_windowed, None, 0, 255, cv2.NORM_MINMAX)\n    \n    # Convert to uint8 and then back to RGB to maintain compatibility\n    # with other functions in your pipeline.\n    return cv2.cvtColor(img_normalized.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n\n\n\ndef apply_stain_normalization(image):\n    \"\"\"\n    Applies Reinhard stain normalization to an RGB image.\n    This function standardizes the color profile of images to make the model\n    more robust to variations in staining.\n    \n    Args:\n        image (numpy.ndarray): An input image in RGB format (uint8).\n        \n    Returns:\n        numpy.ndarray: The stain-normalized image in RGB format (uint8).\n    \"\"\"\n    # Convert image to the LAB color space, which separates color from intensity.\n    # The conversion to float32 is necessary for the color space calculations.\n    img_lab = color.rgb2lab(image.astype(np.float32) / 255.0)\n\n    # These target statistics are a common reference for H&E stained tissue.\n    # They can be fine-tuned if you have a specific, ideal reference image.\n    target_means = [62.4, -0.0001, 0.0001] # Corresponds to L*, a*, b*\n    target_stds = [31.0, 1.0, 1.0]\n\n    # Calculate the means and standard deviations of the source image's channels.\n    src_means = [np.mean(img_lab[:, :, i]) for i in range(3)]\n    src_stds = [np.std(img_lab[:, :, i]) for i in range(3)]\n\n    # Apply the normalization formula to each channel.\n    normalized_lab = np.zeros_like(img_lab)\n    for i in range(3):\n        # (channel - src_mean) / src_std * target_std + target_mean\n        normalized_lab[:, :, i] = (img_lab[:, :, i] - src_means[i]) / (src_stds[i] + 1e-8) * target_stds[i] + target_means[i]\n\n    # Convert the normalized LAB image back to the RGB color space.\n    normalized_rgb = color.lab2rgb(normalized_lab)\n    \n    # Clip values to the valid [0, 1] range and convert back to uint8 [0, 255].\n    return (np.clip(normalized_rgb, 0, 1) * 255).astype(np.uint8)\n\ndef apply_hair_removal(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 17))\n    black_hat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n    _, mask = cv2.threshold(black_hat, 10, 255, cv2.THRESH_BINARY)\n    return cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n\ndef apply_bilateral_filter(image):\n    return cv2.bilateralFilter(image, d=9, sigmaColor=75, sigmaSpace=75)\n\ndef random_erasing(img):\n    if np.random.rand() > 0.5: return img\n    h, w, _ = img.shape\n    x = np.random.randint(0, w)\n    y = np.random.randint(0, h)\n    h_erase = int(h * np.random.uniform(0.05, 0.2))\n    w_erase = int(w * np.random.uniform(0.05, 0.2))\n    img[y:y+h_erase, x:x+w_erase] = np.random.randint(0, 255)\n    return img\n\n\ndef apply_segmentation(image):\n    \"\"\"\n    Segments the image to isolate the main patient anatomy from the background.\n    This version uses Otsu's thresholding for adaptability and morphological\n    operations to clean the resulting mask.\n    \n    Args:\n        image (numpy.ndarray): An input image in RGB format (uint8).\n        \n    Returns:\n        numpy.ndarray: The segmented image with background and noise removed.\n    \"\"\"\n    # Convert to grayscale to create a mask\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # Otsu's thresholding automatically finds the best threshold value\n    # to separate the foreground (body) from the background (black space).\n    _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Use morphological opening to remove small noise artifacts from the mask\n    kernel = np.ones((5, 5), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=2)\n    \n    # Use the cleaned mask to extract the main region of interest\n    segmented_image = cv2.bitwise_and(image, image, mask=mask)\n    \n    return segmented_image\n\ndef apply_clahe(image):\n    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab_image)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    merged_channels = cv2.merge([cl, a, b])\n    return cv2.cvtColor(merged_channels, cv2.COLOR_LAB2RGB)\n\ndef apply_gaussian_blur(image):\n    return cv2.GaussianBlur(image, (5, 5), 0)\n\ndef apply_median_blur(image):\n    return cv2.medianBlur(image, 5)\n\ndef apply_wiener_filter(image):\n    img_float = image.astype(np.float64) / 255.0\n    filtered_channels = [wiener(channel) for channel in cv2.split(img_float)]\n    filtered_image = cv2.merge(filtered_channels)\n    return (np.clip(filtered_image, 0, 1) * 255).astype(np.uint8)\n\ndef apply_histogram_equalization(image):\n    img_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n    img_ycrcb[:, :, 0] = cv2.equalizeHist(img_ycrcb[:, :, 0])\n    return cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\n\ndef apply_laplacian_filter(image):\n    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n    abs_laplacian = np.absolute(laplacian)\n    return np.uint8(abs_laplacian)\n\ndef apply_average_filter(image):\n    return cv2.blur(image, (5, 5))\n\ndef apply_sobel_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n    sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n    sobel_norm = cv2.normalize(sobel_combined, None, 0, 255, cv2.NORM_MINMAX)\n    sobel_uint8 = np.uint8(sobel_norm)\n    return cv2.cvtColor(sobel_uint8, cv2.COLOR_GRAY2RGB)\n\ndef apply_canny_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    edges = cv2.Canny(gray, threshold1=100, threshold2=200)\n    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n\ndef get_preprocessing_function(model_specific_preprocess_input, is_training=False):\n    \"\"\"\n    Creates a master preprocessing function that applies a pipeline of filters\n    and augmentations before applying model-specific scaling.\n    \"\"\"\n    def master_preprocessor(image):\n        # Start with a copy to ensure original data isn't altered unexpectedly.\n        # Convert to uint8 for OpenCV compatibility.\n        processed_image = image.astype('uint8')\n\n        # --- Universal Pre-processing Pipeline ---\n        # These filters are applied to all images for all models.\n        if APPLY_CT_WINDOW:\n            processed_image = apply_ct_window(processed_image)\n        if APPLY_STAIN_NORMALIZATION:\n            processed_image = apply_stain_normalization(processed_image)\n        if APPLY_HAIR_REMOVAL:\n            processed_image = apply_hair_removal(processed_image)\n        if APPLY_BILATERAL_FILTER:\n            processed_image = apply_bilateral_filter(processed_image)\n        if APPLY_SEGMENTATION:\n            processed_image = apply_segmentation(processed_image)\n        if APPLY_WIENER_FILTER:\n            processed_image = apply_wiener_filter(processed_image)\n        if APPLY_MEDIAN_FILTER:\n            processed_image = apply_median_blur(processed_image)\n        if APPLY_AVERAGE_FILTER:\n            processed_image = apply_average_filter(processed_image)\n        if APPLY_GAUSSIAN_BLUR:\n            processed_image = apply_gaussian_blur(processed_image)\n        if APPLY_CLAHE:\n            processed_image = apply_clahe(processed_image)\n        if APPLY_HISTOGRAM_EQUALIZATION:\n            processed_image = apply_histogram_equalization(processed_image)\n        if APPLY_LAPLACIAN_FILTER:\n            processed_image = apply_laplacian_filter(processed_image)\n        if APPLY_SOBEL_FILTER:\n            processed_image = apply_sobel_filter(processed_image)\n        if APPLY_CANNY_FILTER:\n            processed_image = apply_canny_filter(processed_image)\n\n        # Convert to float32 for augmentation and model input\n        processed_image = processed_image.astype('float32')\n\n        # --- Training-Only Augmentation ---\n        if is_training and APPLY_RANDOM_ERASING:\n            processed_image = random_erasing(processed_image)\n\n        # --- Final Model-Specific Processing ---\n        # This is the last step. It applies the specific function for pre-trained\n        # models or a simple rescale for the CustomCNN.\n        if model_specific_preprocess_input:\n            return model_specific_preprocess_input(processed_image)\n        else:\n            return processed_image / 255.0\n            \n    return master_preprocessor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Function to Create the Custom CNN Model ---","metadata":{}},{"cell_type":"code","source":"def create_custom_cnn(input_shape, num_output_units, last_layer_activation):\n    \"\"\"Builds and returns the user-defined custom CNN model.\"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),\n        layers.Conv2D(16, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(32, (1, 1), activation='relu'),\n        layers.Conv2D(32, (3, 3), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(64, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (5, 5), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(16, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(16, (5, 5), activation='relu'),\n        layers.BatchNormalization(),\n        # Add a feature extractor layer name for Grad-CAM and feature projection\n        layers.GlobalAveragePooling2D(name='feature_extractor_layer'),\n        layers.Dense(num_output_units, activation=last_layer_activation)\n    ])\n    print(\"--- Custom CNN Model Summary ---\")\n    model.summary()\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 3. VISUALIZATION AND REPORTING FUNCTIONS ---","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history, model_name, save_dir):\n    \"\"\"Plots accuracy, loss, and precision from the model's history.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n    metrics = ['accuracy', 'loss', 'precision']\n    for i, metric in enumerate(metrics):\n        val_metric = f'val_{metric}'\n        axes[i].plot(history.history[metric], label=f'Train {metric.capitalize()}')\n        axes[i].plot(history.history[val_metric], label=f'Validation {metric.capitalize()}')\n        axes[i].set_title(f'{model_name} - {metric.capitalize()}')\n        axes[i].legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_training_history.png'))\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, model_name, save_dir):\n    \"\"\"Plots a confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'))\n    plt.show()\n\ndef plot_roc_pr_curves(y_true, y_pred_prob, class_names, model_name, save_dir):\n    \"\"\"\n    Plots ROC and Precision-Recall curves for binary AND multiclass classification.\n    For multiclass, it uses the One-vs-Rest (OvR) strategy.\n    \"\"\"\n    n_classes = len(class_names)\n    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\n    # --- ROC Curve (Binary and Multiclass) ---\n    if n_classes == 2:\n        # Standard binary case\n        fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n        roc_auc = auc(fpr, tpr)\n        axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    else:\n        # Multiclass case (One-vs-Rest)\n        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Plot each class's ROC curve\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n        for i, color in zip(range(n_classes), colors):\n            axes[0].plot(fpr[i], tpr[i], color=color, lw=2,\n                         label=f'ROC curve of {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n\n    axes[0].plot([0, 1], [0, 1], 'k--', lw=2)\n    axes[0].set_xlim([0.0, 1.0])\n    axes[0].set_ylim([0.0, 1.05])\n    axes[0].set_xlabel('False Positive Rate')\n    axes[0].set_ylabel('True Positive Rate')\n    axes[0].set_title(f'{model_name} - Receiver Operating Characteristic')\n    axes[0].legend(loc=\"lower right\")\n\n    # --- Precision-Recall Curve (Binary and Multiclass) ---\n    if n_classes == 2:\n        # Standard binary case\n        precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n        pr_auc = auc(recall, precision)\n        axes[1].plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n    else:\n        # Multiclass case (One-vs-Rest)\n        precision = dict()\n        recall = dict()\n        pr_auc = dict()\n\n        for i in range(n_classes):\n            precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_prob[:, i])\n            pr_auc[i] = auc(recall[i], precision[i])\n\n        # Plot each class's PR curve\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n        for i, color in zip(range(n_classes), colors):\n            axes[1].plot(recall[i], precision[i], color=color, lw=2,\n                         label=f'PR curve of {class_names[i]} (AP = {pr_auc[i]:.2f})')\n\n    axes[1].set_xlim([0.0, 1.0])\n    axes[1].set_ylim([0.0, 1.05])\n    axes[1].set_xlabel('Recall')\n    axes[1].set_ylabel('Precision')\n    axes[1].set_title(f'{model_name} - Precision-Recall Curve')\n    axes[1].legend(loc=\"lower left\")\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_roc_pr_curves.png'))\n    plt.show()\n\ndef plot_projections(features, labels, class_names, model_name, save_dir):\n    \"\"\"Plots t-SNE and UMAP projections of features.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n    \n    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1)).fit_transform(features)\n    df_tsne = pd.DataFrame({'x': tsne[:, 0], 'y': tsne[:, 1], 'label': [class_names[l] for l in labels]})\n    sns.scatterplot(data=df_tsne, x='x', y='y', hue='label', ax=axes[0], palette='viridis').set_title(f'{model_name} - t-SNE')\n    \n    umap_proj = UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(features)\n    df_umap = pd.DataFrame({'x': umap_proj[:, 0], 'y': umap_proj[:, 1], 'label': [class_names[l] for l in labels]})\n    sns.scatterplot(data=df_umap, x='x', y='y', hue='label', ax=axes[1], palette='viridis').set_title(f'{model_name} - UMAP')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_projections.png'))\n    plt.show()\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n    \"\"\"Creates a Grad-CAM heatmap.\"\"\"\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        # For binary with sigmoid, the class channel is the output itself.\n        class_channel = preds\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    heatmap = last_conv_layer_output[0] @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path, alpha=0.4):\n    \"\"\"Saves a superimposed Grad-CAM image.\"\"\"\n    img = cv2.imread(img_path); img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])); heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = np.clip(heatmap * alpha + img, 0, 255).astype('uint8')\n    cv2.imwrite(cam_path, superimposed_img)\n\ndef visualize_class_maps(model, last_conv_layer_name, preprocessor, model_name, save_dir, test_dir_path):\n    \"\"\"Displays Grad-CAM for one sample from each class in a 2xN layout.\"\"\"\n    class_names = sorted(os.listdir(test_dir_path))\n    num_classes = len(class_names)\n\n    # --- CHANGE 1: Define grid layout ---\n    nrows = 2\n    # Calculate columns needed to fit all classes, rounding up\n    ncols = math.ceil(num_classes / nrows) \n\n    # Define how many inches you want for each subplot's width and height\n    subplot_width_inches = 3\n    subplot_height_inches = 3.5 # Slightly more height for titles\n\n    # --- CHANGE 2: Update figsize calculation for the new grid ---\n    fig_width = ncols * subplot_width_inches\n    fig_height = nrows * subplot_height_inches\n    \n    plt.figure(figsize=(fig_width, fig_height))\n    # Add a main title for the entire figure\n    plt.suptitle(f'Grad-CAM Class Activation Maps for {model_name}', fontsize=16)\n\n    for i, class_name in enumerate(class_names):\n        img_path = os.path.join(test_dir_path, class_name, os.listdir(os.path.join(test_dir_path, class_name))[0])\n        img_array = img_to_array(load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH)))\n        img_preprocessed = preprocessor(img_array.copy()) if preprocessor else img_array / 255.0\n        img_for_model = np.expand_dims(img_preprocessed, axis=0)\n        \n        heatmap = make_gradcam_heatmap(img_for_model, model, last_conv_layer_name)\n        cam_path = os.path.join(save_dir, f'{model_name}_gradcam_{class_name}.png')\n        save_and_display_gradcam(img_path, heatmap, cam_path)\n\n        # --- CHANGE 3: Update subplot call with new rows and columns ---\n        ax = plt.subplot(nrows, ncols, i + 1)\n        ax.imshow(cv2.cvtColor(cv2.imread(cam_path), cv2.COLOR_BGR2RGB))\n        ax.set_title(f'Grad-CAM: {class_name}',fontsize=8)\n        ax.axis(\"off\")\n\n    # Adjust layout to prevent title overlap and show the plot\n    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust rect to make space for suptitle\n    plt.show()\n\ndef visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, save_dir, num_examples_per_class=2):\n    \"\"\"Shows sample predictions, highlighting correct and incorrect ones.\"\"\"\n    filenames = test_generator.filenames\n    examples_shown = {name: 0 for name in class_names}\n    # Define how many inches you want for each subplot's width and height\n    subplot_width_inches = 3\n    subplot_height_inches = 3\n    \n    # Calculate the total figure size\n    fig_width = len(class_names) * subplot_width_inches\n    fig_height = num_examples_per_class * subplot_height_inches\n    \n    fig, axes = plt.subplots(\n        nrows=num_examples_per_class, \n        ncols=len(class_names), \n        figsize=(fig_width, fig_height), \n        squeeze=False\n    )\n    fig.suptitle(f'{model_name} - Prediction Samples', fontsize=16)\n    for i in range(len(filenames)):\n        if all(v >= num_examples_per_class for v in examples_shown.values()): break\n        true_label_idx = y_true[i]\n        true_label_name = class_names[true_label_idx]\n        if examples_shown[true_label_name] < num_examples_per_class:\n            img_path = os.path.join(test_generator.directory, filenames[i])\n            ax = axes[examples_shown[true_label_name], true_label_idx]\n            ax.imshow(load_img(img_path))\n            ax.axis('off')\n            title_color = 'green' if y_pred[i] == true_label_idx else 'red'\n            ax.set_title(f\"True: {true_label_name}\\nPred: {class_names[y_pred[i]]}\", color=title_color, fontsize=8)\n            examples_shown[true_label_name] += 1\n    fig.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(os.path.join(save_dir, f'{model_name}_prediction_samples.png')); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 4. MAIN TRAINING & EVALUATION LOOP ---","metadata":{}},{"cell_type":"markdown","source":" # --- Evaluation & Visualization ---","metadata":{}},{"cell_type":"code","source":"# --- Run the Setup ---\nsetup_directories()\n\n# You can now use these variables to point to your new dataset\ntrain_dir = os.path.join(BASE_DIR, 'train')\nval_dir = os.path.join(BASE_DIR, 'val')\ntest_dir = os.path.join(BASE_DIR, 'test')\n\nNUM_CLASSES = len(os.listdir(train_dir))\n\nif APPLY_GAN_AUGMENTATION and NUM_CLASSES >= 2:\n    for class_name in CLASSES_TO_AUGMENT:\n        if class_name in TARGET_CLASSES:\n            train_gan_and_generate_images(class_name, NUM_IMAGES_TO_GENERATE, train_dir)\n        else:\n            print(f\"WARNING: Class '{class_name}' for GAN is not in TARGET_CLASSES. Skipping.\")\n\n\n# --- Model Registry ---\nMODELS = {\n    'CustomCNN': (None, None),\n    'VGG16': (VGG16, tf.keras.applications.vgg16.preprocess_input),\n    'VGG19': (VGG19, tf.keras.applications.vgg19.preprocess_input),\n    'ResNet50': (ResNet50, tf.keras.applications.resnet50.preprocess_input),\n    'InceptionV3': (InceptionV3, tf.keras.applications.inception_v3.preprocess_input),\n    'Xception': (Xception, tf.keras.applications.xception.preprocess_input),\n    'MobileNetV2': (MobileNetV2, tf.keras.applications.mobilenet_v2.preprocess_input),\n    'DenseNet121': (DenseNet121, tf.keras.applications.densenet.preprocess_input),\n    'EfficientNetB0': (EfficientNetB0, tf.keras.applications.efficientnet.preprocess_input),\n    'InceptionResNetV2': (InceptionResNetV2, tf.keras.applications.inception_resnet_v2.preprocess_input),\n}\n\n# --- NEW: Initialize a list to store metrics from all models ---\nall_models_metrics = []\n\n\n# --- Main Training and Evaluation Loop ---\nif NUM_CLASSES >= 2:\n    # --- DYNAMIC CONFIGURATION BASED ON CLASS COUNT ---\n    if NUM_CLASSES == 2:\n        class_mode = 'binary'\n        loss_function = 'binary_crossentropy'\n        last_layer_activation = 'sigmoid'\n        num_output_units = 1\n    else:  # Multi-class\n        class_mode = 'categorical'\n        loss_function = 'categorical_crossentropy'\n        last_layer_activation = 'softmax'\n        num_output_units = NUM_CLASSES\n\n    print(f\"\\n--- Setting up training for {NUM_CLASSES} classes ---\")\n    print(f\"Class Mode: {class_mode}, Loss: {loss_function}, Activation: {last_layer_activation}\")\n\n    for model_name, (model_constructor, preprocess_input) in MODELS.items():\n        print(f\"\\n{'='*25} Training and Evaluating: {model_name} {'='*25}\")\n        model_save_dir = os.path.join(OUTPUT_DIR, model_name)\n        os.makedirs(model_save_dir, exist_ok=True)\n\n        # ******************************************************************\n        # --- SECTION CHANGED: Corrected Data Generators ---\n        # This implementation now correctly uses your custom get_preprocessing_function.\n        print(f\"Instantiating preprocessor for {model_name}...\")\n        \n        # For CustomCNN, pass `None`. get_preprocessing_function handles this.\n        model_specific_preprocessing = None if model_name == 'CustomCNN' else preprocess_input\n\n        # Create the master preprocessor for training data (with augmentations)\n        train_preprocessor = get_preprocessing_function(\n            model_specific_preprocess_input=model_specific_preprocessing,\n            is_training=True  # Enables training-only augmentations\n        )\n\n        # Create the master preprocessor for validation/test data (no augmentations)\n        val_test_preprocessor = get_preprocessing_function(\n            model_specific_preprocess_input=model_specific_preprocessing,\n            is_training=False # Disables augmentations for consistent evaluation\n        )\n\n        # Use these master preprocessors in your ImageDataGenerator\n        train_datagen = ImageDataGenerator(preprocessing_function=train_preprocessor)\n        val_test_datagen = ImageDataGenerator(preprocessing_function=val_test_preprocessor)\n        # ******************************************************************\n\n\n        train_generator = train_datagen.flow_from_directory(\n            train_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n            batch_size=BATCH_SIZE, class_mode=class_mode\n        )\n        validation_generator = val_test_datagen.flow_from_directory(\n            val_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n            batch_size=BATCH_SIZE, class_mode=class_mode\n        )\n        test_generator = val_test_datagen.flow_from_directory(\n            test_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n            batch_size=BATCH_SIZE, class_mode=class_mode, shuffle=False\n        )\n\n        # --- Model Building ---\n        if model_name == 'CustomCNN':\n            model = create_custom_cnn((IMG_HEIGHT, IMG_WIDTH, 3), num_output_units, last_layer_activation)\n        else:\n            base_model = model_constructor(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n            base_model.trainable = False\n            x = GlobalAveragePooling2D(name='feature_extractor_layer')(base_model.output)\n            x = Dense(128, activation='relu')(x)\n            x = Dropout(0.5)(x)\n            predictions = Dense(num_output_units, activation=last_layer_activation)(x)\n            model = Model(inputs=base_model.input, outputs=predictions)\n\n        # --- Model Training ---\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=loss_function, metrics=['accuracy', Precision(name='precision')])\n        best_model_path = os.path.join(model_save_dir, f'{model_name}_best.keras')\n        callbacks = [\n            EarlyStopping(monitor='val_accuracy', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True),\n            ModelCheckpoint(filepath=best_model_path, save_best_only=True, monitor='val_accuracy')\n        ]\n        history = model.fit(train_generator, epochs=EPOCHS, validation_data=validation_generator, callbacks=callbacks)\n\n        # --- Evaluation & Visualization ---\n        print(f\"\\n--- Loading best model from '{best_model_path}' for evaluation ---\")\n        model = tf.keras.models.load_model(best_model_path)\n        plot_training_history(history, model_name, model_save_dir)\n\n        y_pred_prob = model.predict(test_generator)\n        y_true = test_generator.classes\n        class_names = list(test_generator.class_indices.keys())\n\n        if class_mode == 'binary':\n            y_pred = (y_pred_prob.flatten() > 0.5).astype(int)\n        else: # categorical\n            y_pred = np.argmax(y_pred_prob, axis=1)\n\n        print(f'\\nClassification Report for {model_name}:\\n')\n        print(classification_report(y_true, y_pred, target_names=class_names, digits=2))\n\n        mcc = matthews_corrcoef(y_true, y_pred)\n        print(f\"Overall Matthew's Correlation Coefficient (MCC): {mcc:.2f}\\n\")\n\n        cm = confusion_matrix(y_true, y_pred)\n        report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n\n        metrics_data = []\n        for i, class_name in enumerate(class_names):\n            TP = cm[i, i]\n            FP = cm[:, i].sum() - TP\n            FN = cm[i, :].sum() - TP\n            TN = cm.sum() - (TP + FP + FN)\n            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n            \n            metrics_data.append({\n                \"Class\": class_name,\n                \"Precision\": report_dict[class_name]['precision'],\n                \"Recall (Sensitivity)\": report_dict[class_name]['recall'],\n                \"F1-Score\": report_dict[class_name]['f1-score'],\n                \"Specificity\": specificity\n            })\n\n        metrics_df = pd.DataFrame(metrics_data)\n        print(\"Detailed Per-Class Metrics Summary:\")\n        print(metrics_df.to_string(index=False, float_format=\"%.4f\"))\n\n        avg_specificity = metrics_df['Specificity'].mean()\n        print(f\"\\nAverage Specificity: {avg_specificity:.4f}\")\n\n        # --- NEW: Store the overall metrics for the final table ---\n        model_metrics = {\n            'Model Name': model_name,\n            'Accuracy': report_dict['accuracy'],\n            'Precision': report_dict['weighted avg']['precision'],\n            'Recall': report_dict['weighted avg']['recall'],\n            'F1-Score': report_dict['weighted avg']['f1-score'],\n            'Specificity': avg_specificity,\n            'MCC': mcc\n        }\n        all_models_metrics.append(model_metrics)\n\n        plot_confusion_matrix(y_true, y_pred, class_names, model_name, model_save_dir)\n\n        y_pred_for_curves = y_pred_prob.flatten() if class_mode == 'binary' else y_pred_prob\n        plot_roc_pr_curves(y_true, y_pred_for_curves, class_names, model_name, model_save_dir)\n\n        feature_extractor = Model(inputs=model.inputs, outputs=model.get_layer('feature_extractor_layer').output)\n        test_features = feature_extractor.predict(test_generator)\n        plot_projections(test_features, y_true, class_names, model_name, model_save_dir)\n\n        last_conv_layer_name = next((layer.name for layer in reversed(model.layers) if 'conv' in layer.name.lower()), None)\n        # For Grad-CAM, pass the raw preprocessor for the model, not the one with augmentations\n        grad_cam_preprocessor = get_preprocessing_function(model_specific_preprocessing, is_training=False)\n        if last_conv_layer_name and model_name != 'CustomCNN':\n            print(f\"Generating Grad-CAM for {model_name} using layer: {last_conv_layer_name}\")\n            visualize_class_maps(model, last_conv_layer_name, grad_cam_preprocessor, model_name, model_save_dir, test_dir)\n        else:\n            print(f\"Skipping Grad-CAM visualization for {model_name}.\")\n\n        visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, model_save_dir)\n        print(f\"\\nFinished processing {model_name}. Results saved to {model_save_dir}\")\n\n    print(\"\\nAll models have been trained and evaluated.\")\n\nelse:\n    print(\"\\nSkipping model training because a valid dataset with 2 or more classes was not created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib.backends.backend_pdf import PdfPages\n\n# --- NEW: PDF Generation Section ---\n# This section runs after all models have been evaluated.\n\nif all_models_metrics:\n    print(\"\\n--- Generating Final Performance Report PDF ---\")\n    \n    # Convert the list of dictionaries to a pandas DataFrame\n    results_df = pd.DataFrame(all_models_metrics)\n\n    # Format float columns to 4 decimal places for display\n    float_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity', 'MCC']\n    for col in float_cols:\n        results_df[col] = results_df[col].map('{:.4f}'.format)\n\n    # Function to render the DataFrame as a styled table in Matplotlib\n    def render_mpl_table(data, col_width=2.5, row_height=0.625, font_size=12,\n                         header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n                         bbox=[0, 0, 1, 1], header_columns=0, ax=None, **kwargs):\n        if ax is None:\n            size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n            fig, ax = plt.subplots(figsize=size)\n            ax.axis('off')\n\n        mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n        mpl_table.auto_set_font_size(False)\n        mpl_table.set_fontsize(font_size)\n\n        for k, cell in mpl_table._cells.items():\n            cell.set_edgecolor(edge_color)\n            if k[0] == 0:  # Header row\n                cell.set_text_props(weight='bold', color='w')\n                cell.set_facecolor(header_color)\n            else:  # Data rows\n                cell.set_facecolor(row_colors[k[0] % len(row_colors)])\n        return ax\n\n    # Define the output PDF file path\n    output_pdf_path = os.path.join(OUTPUT_DIR, 'model_performance_report.pdf')\n\n    with PdfPages(output_pdf_path) as pdf:\n        fig, ax = plt.subplots(figsize=(16, 4))\n        ax.axis('off')\n        ax.set_title('Model Performance Metrics', fontsize=16, weight='bold', pad=20)\n        \n        render_mpl_table(results_df, header_columns=0, col_width=2.2, ax=ax)\n        \n        pdf.savefig(fig, bbox_inches='tight')\n        plt.close(fig)\n\n    print(f\"\\nTable of results successfully saved to: {output_pdf_path}\")\nelse:\n    print(\"\\nNo models were evaluated, skipping PDF report generation.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}