{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:06.714291Z",
     "iopub.status.busy": "2025-08-05T11:03:06.713944Z",
     "iopub.status.idle": "2025-08-05T11:03:06.720776Z",
     "shell.execute_reply": "2025-08-05T11:03:06.720051Z",
     "shell.execute_reply.started": "2025-08-05T11:03:06.714252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:06.722117Z",
     "iopub.status.busy": "2025-08-05T11:03:06.721912Z",
     "iopub.status.idle": "2025-08-05T11:03:39.958212Z",
     "shell.execute_reply": "2025-08-05T11:03:39.957432Z",
     "shell.execute_reply.started": "2025-08-05T11:03:06.722101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 11:03:09.276544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754391789.449128      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754391789.503202      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from skimage import color\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from scipy.signal import wiener\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, matthews_corrcoef\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16, VGG19, ResNet50, InceptionV3, Xception, MobileNetV2, \n",
    "    DenseNet121, EfficientNetB0, InceptionResNetV2\n",
    ")\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Average\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 1. CONFIGURATION AND SETUP ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Dataset and Output Paths ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:39.959596Z",
     "iopub.status.busy": "2025-08-05T11:03:39.959047Z",
     "iopub.status.idle": "2025-08-05T11:03:39.963316Z",
     "shell.execute_reply": "2025-08-05T11:03:39.962658Z",
     "shell.execute_reply.started": "2025-08-05T11:03:39.959574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SOURCE_DIR = '/kaggle/input/multicancer-dataset/Cancer_Dataset'\n",
    "BASE_DIR = '/kaggle/working/cervix_cancer_dataset'\n",
    "OUTPUT_DIR = '/kaggle/working/model_outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Model and Training Parameters ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:39.965121Z",
     "iopub.status.busy": "2025-08-05T11:03:39.964888Z",
     "iopub.status.idle": "2025-08-05T11:03:39.986852Z",
     "shell.execute_reply": "2025-08-05T11:03:39.986120Z",
     "shell.execute_reply.started": "2025-08-05T11:03:39.965100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "EARLY_STOPPING_PATIENCE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:39.987754Z",
     "iopub.status.busy": "2025-08-05T11:03:39.987523Z",
     "iopub.status.idle": "2025-08-05T11:03:40.001854Z",
     "shell.execute_reply": "2025-08-05T11:03:40.001087Z",
     "shell.execute_reply.started": "2025-08-05T11:03:39.987734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLASS_MAP = {\n",
    "    \"cervix_dyk\": \"Dyskeratosis\",\n",
    "    \"cervix_koc\": \"Koilocytes\",\n",
    "    \"cervix_mep\": \"Metaplastic_cells\",\n",
    "    \"cervix_pab\": \"Parabasal_cells\",\n",
    "    \"cervix_sfi\": \"Superficial_Intermediate_cells\"\n",
    "}\n",
    "# Data_Selector = 'cervix_'\n",
    "# # NEW: This will be set dynamically based on the folders found.\n",
    "NUM_CLASSES = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Preprocessing Toggles ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.003262Z",
     "iopub.status.busy": "2025-08-05T11:03:40.002607Z",
     "iopub.status.idle": "2025-08-05T11:03:40.015487Z",
     "shell.execute_reply": "2025-08-05T11:03:40.014796Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.003233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "APPLY_SEGMENTATION = False\n",
    "APPLY_CLAHE = False\n",
    "APPLY_GAUSSIAN_BLUR = False \n",
    "APPLY_MEDIAN_FILTER = False\n",
    "APPLY_WIENER_FILTER = False\n",
    "APPLY_HISTOGRAM_EQUALIZATION = False\n",
    "APPLY_LAPLACIAN_FILTER = False\n",
    "APPLY_AVERAGE_FILTER = False\n",
    "APPLY_SOBEL_FILTER = False\n",
    "APPLY_CANNY_FILTER = False\n",
    "APPLY_STAIN_NORMALIZATION = False\n",
    "\n",
    "# --- Advanced Pre-processing & Augmentation Flags ---\n",
    "APPLY_HAIR_REMOVAL = False\n",
    "APPLY_BILATERAL_FILTER = False\n",
    "APPLY_RANDOM_ERASING = False\n",
    "APPLY_GAN_AUGMENTATION = False # Master switch for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.016456Z",
     "iopub.status.busy": "2025-08-05T11:03:40.016235Z",
     "iopub.status.idle": "2025-08-05T11:03:40.032364Z",
     "shell.execute_reply": "2025-08-05T11:03:40.031667Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.016435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- GAN-Specific Configuration (MODIFIED) ---\n",
    "if APPLY_GAN_AUGMENTATION:\n",
    "    CLASSES_TO_AUGMENT = TARGET_CLASSES\n",
    "    NUM_IMAGES_TO_GENERATE = 100\n",
    "    GAN_IMG_SIZE = 224  # MODIFIED: Changed to 224 to match the classifier\n",
    "    LATENT_DIM = 100\n",
    "    GAN_EPOCHS = 75 # Increased epochs slightly for the more complex generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.033345Z",
     "iopub.status.busy": "2025-08-05T11:03:40.033111Z",
     "iopub.status.idle": "2025-08-05T11:03:40.049357Z",
     "shell.execute_reply": "2025-08-05T11:03:40.048661Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.033325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"\n",
    "    Collects images from 'train' and 'val' source subfolders, and then\n",
    "    creates a new, cleanly named dataset with a fresh train/val/test split.\n",
    "    \"\"\"\n",
    "    # 1. Clean up old directory and create the new structure.\n",
    "    if os.path.exists(BASE_DIR):\n",
    "        shutil.rmtree(BASE_DIR)\n",
    "    \n",
    "    train_dir = os.path.join(BASE_DIR, 'train')\n",
    "    val_dir = os.path.join(BASE_DIR, 'val')\n",
    "    test_dir = os.path.join(BASE_DIR, 'test')\n",
    "    os.makedirs(train_dir); os.makedirs(val_dir); os.makedirs(test_dir)\n",
    "\n",
    "    print(f\"New dataset will be created at: {BASE_DIR}\")\n",
    "\n",
    "    # 2. Iterate through your class mapping.\n",
    "    for source_folder, dest_folder in CLASS_MAP.items():\n",
    "        print(f\"\\nProcessing: '{source_folder}' -> '{dest_folder}'\")\n",
    "        \n",
    "        all_files = []\n",
    "        \n",
    "        # 3. Collect all files from BOTH the train and val directories.\n",
    "        for subfolder in ['train', 'val']:\n",
    "            src_path = os.path.join(SOURCE_DIR, subfolder, source_folder)\n",
    "            \n",
    "            if os.path.exists(src_path):\n",
    "                files_found = [os.path.join(src_path, f) for f in os.listdir(src_path)\n",
    "                               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "                all_files.extend(files_found)\n",
    "                print(f\"  - Found {len(files_found)} images in '{subfolder}' directory.\")\n",
    "            else:\n",
    "                print(f\"  - WARNING: Directory not found, skipping: {src_path}\")\n",
    "\n",
    "        if not all_files:\n",
    "            print(f\"  - No images found for this class. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # 4. Create the new destination folders (e.g., 'train/Dyskeratosis').\n",
    "        for d in [train_dir, val_dir, test_dir]:\n",
    "            os.makedirs(os.path.join(d, dest_folder), exist_ok=True)\n",
    "            \n",
    "        # 5. Shuffle and split the combined list of files.\n",
    "        np.random.shuffle(all_files)\n",
    "        train_end = int(len(all_files) * 0.7)\n",
    "        val_end = train_end + int(len(all_files) * 0.15)\n",
    "        \n",
    "        train_files = all_files[:train_end]\n",
    "        val_files = all_files[train_end:val_end]\n",
    "        test_files = all_files[val_end:]\n",
    "        \n",
    "        # 6. Copy the files into their new homes.\n",
    "        for f_path in train_files: shutil.copy(f_path, os.path.join(train_dir, dest_folder, os.path.basename(f_path)))\n",
    "        for f_path in val_files: shutil.copy(f_path, os.path.join(val_dir, dest_folder, os.path.basename(f_path)))\n",
    "        for f_path in test_files: shutil.copy(f_path, os.path.join(test_dir, dest_folder, os.path.basename(f_path)))\n",
    "            \n",
    "    print(\"\\nDataset setup and file copying complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 2. PREPROCESSING FUNCTIONS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.050464Z",
     "iopub.status.busy": "2025-08-05T11:03:40.050226Z",
     "iopub.status.idle": "2025-08-05T11:03:40.075053Z",
     "shell.execute_reply": "2025-08-05T11:03:40.074424Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.050441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "    \"\"\"\n",
    "    Builds the DCGAN Generator model, re-architected to output 224x224 images.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Start with a 7x7 spatial size\n",
    "        layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(latent_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "\n",
    "        # Upsampling block 1: 7x7 -> 14x14\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        # Upsampling block 2: 14x14 -> 28x28\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        # Upsampling block 3: 28x28 -> 56x56\n",
    "        layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        # Upsampling block 4: 56x56 -> 112x112\n",
    "        layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        \n",
    "        # Upsampling block 5: 112x112 -> 224x224\n",
    "        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'),\n",
    "        # Final output shape: (224, 224, 3)\n",
    "    ], name=\"generator\")\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    \"\"\"Builds the DCGAN Discriminator model.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=input_shape),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1) # Logit output\n",
    "    ], name=\"discriminator\")\n",
    "    return model\n",
    "\n",
    "def train_gan_and_generate_images(class_name, num_to_generate, base_train_dir):\n",
    "    \"\"\"Trains a DCGAN on a specific class and saves generated images to the training folder.\"\"\"\n",
    "    print(f\"\\n--- Starting GAN Augmentation for class: {class_name} ---\")\n",
    "    target_dir = os.path.join(base_train_dir, class_name)\n",
    "    if not os.path.exists(target_dir):\n",
    "        print(f\"ERROR: Cannot find source directory for GAN training: {target_dir}\")\n",
    "        return\n",
    "\n",
    "    # 1. Load Data, resizing to the new GAN_IMG_SIZE\n",
    "    real_images = []\n",
    "    for filename in os.listdir(target_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img = load_img(os.path.join(target_dir, filename), target_size=(GAN_IMG_SIZE, GAN_IMG_SIZE))\n",
    "            real_images.append(img_to_array(img))\n",
    "    \n",
    "    if len(real_images) < BATCH_SIZE:\n",
    "        print(f\"WARNING: Not enough images ({len(real_images)}) in {target_dir} to train GAN with batch size {BATCH_SIZE}. Skipping.\")\n",
    "        return\n",
    "        \n",
    "    real_images = (np.array(real_images) - 127.5) / 127.5\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(real_images).shuffle(len(real_images)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    # 2. Build Models with the correct input shape\n",
    "    generator = build_generator(LATENT_DIM)\n",
    "    discriminator = build_discriminator((GAN_IMG_SIZE, GAN_IMG_SIZE, 3))\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def discriminator_loss(real_output, fake_output):\n",
    "        return cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    def generator_loss(fake_output):\n",
    "        return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    gen_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)\n",
    "    disc_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(images):\n",
    "        noise = tf.random.normal([images.shape[0], LATENT_DIM])\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_images = generator(noise, training=True)\n",
    "            real_output = discriminator(images, training=True)\n",
    "            fake_output = discriminator(generated_images, training=True)\n",
    "            gen_loss = generator_loss(fake_output)\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
    "        disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # 3. Training Loop\n",
    "    print(f\"Training GAN for {GAN_EPOCHS} epochs on {GAN_IMG_SIZE}x{GAN_IMG_SIZE} images...\")\n",
    "    for epoch in range(GAN_EPOCHS):\n",
    "        for image_batch in train_dataset:\n",
    "            train_step(image_batch)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  - GAN Epoch {epoch + 1}/{GAN_EPOCHS} completed.\")\n",
    "\n",
    "    # 4. Generate and Save Images\n",
    "    print(\"Generating synthetic images...\")\n",
    "    noise = tf.random.normal([num_to_generate, LATENT_DIM])\n",
    "    generated_images = generator(noise, training=False)\n",
    "    generated_images = (generated_images * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "    \n",
    "    for i, img_array in enumerate(generated_images):\n",
    "        img = array_to_img(img_array)\n",
    "        # REMOVED: No longer need to resize, as the GAN generates at the correct size\n",
    "        img.save(os.path.join(target_dir, f'synthetic_gan_{i+1}.png'))\n",
    "        \n",
    "    print(f\"Successfully added {num_to_generate} synthetic images to '{target_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.077332Z",
     "iopub.status.busy": "2025-08-05T11:03:40.077122Z",
     "iopub.status.idle": "2025-08-05T11:03:40.100432Z",
     "shell.execute_reply": "2025-08-05T11:03:40.099742Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.077316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_stain_normalization(image):\n",
    "    \"\"\"\n",
    "    Applies Reinhard stain normalization to an RGB image.\n",
    "    This function standardizes the color profile of images to make the model\n",
    "    more robust to variations in staining.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): An input image in RGB format (uint8).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The stain-normalized image in RGB format (uint8).\n",
    "    \"\"\"\n",
    "    # Convert image to the LAB color space, which separates color from intensity.\n",
    "    # The conversion to float32 is necessary for the color space calculations.\n",
    "    img_lab = color.rgb2lab(image.astype(np.float32) / 255.0)\n",
    "\n",
    "    # These target statistics are a common reference for H&E stained tissue.\n",
    "    # They can be fine-tuned if you have a specific, ideal reference image.\n",
    "    target_means = [62.4, -0.0001, 0.0001] # Corresponds to L*, a*, b*\n",
    "    target_stds = [31.0, 1.0, 1.0]\n",
    "\n",
    "    # Calculate the means and standard deviations of the source image's channels.\n",
    "    src_means = [np.mean(img_lab[:, :, i]) for i in range(3)]\n",
    "    src_stds = [np.std(img_lab[:, :, i]) for i in range(3)]\n",
    "\n",
    "    # Apply the normalization formula to each channel.\n",
    "    normalized_lab = np.zeros_like(img_lab)\n",
    "    for i in range(3):\n",
    "        # (channel - src_mean) / src_std * target_std + target_mean\n",
    "        normalized_lab[:, :, i] = (img_lab[:, :, i] - src_means[i]) / (src_stds[i] + 1e-8) * target_stds[i] + target_means[i]\n",
    "\n",
    "    # Convert the normalized LAB image back to the RGB color space.\n",
    "    normalized_rgb = color.lab2rgb(normalized_lab)\n",
    "    \n",
    "    # Clip values to the valid [0, 1] range and convert back to uint8 [0, 255].\n",
    "    return (np.clip(normalized_rgb, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "def apply_hair_removal(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 17))\n",
    "    black_hat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
    "    _, mask = cv2.threshold(black_hat, 10, 255, cv2.THRESH_BINARY)\n",
    "    return cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n",
    "\n",
    "def apply_bilateral_filter(image):\n",
    "    return cv2.bilateralFilter(image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "def random_erasing(img):\n",
    "    if np.random.rand() > 0.5: return img\n",
    "    h, w, _ = img.shape\n",
    "    x = np.random.randint(0, w)\n",
    "    y = np.random.randint(0, h)\n",
    "    h_erase = int(h * np.random.uniform(0.05, 0.2))\n",
    "    w_erase = int(w * np.random.uniform(0.05, 0.2))\n",
    "    img[y:y+h_erase, x:x+w_erase] = np.random.randint(0, 255)\n",
    "    return img\n",
    "\n",
    "\n",
    "def apply_segmentation(image):\n",
    "    \"\"\"\n",
    "    Segments the image to isolate key cellular structures.\n",
    "    This version uses a fixed grayscale threshold, which is often more reliable\n",
    "    for cytology images than Otsu's method, as it focuses on capturing the\n",
    "    darker nuclei without being influenced by background variations.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): An input image in RGB format (uint8).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The segmented image with background removed.\n",
    "    \"\"\"\n",
    "    # Convert the image to grayscale to work with intensity values.\n",
    "    gray = color.rgb2gray(image)\n",
    "    \n",
    "    # Apply a fixed threshold. Pixels darker than this value (likely nuclei)\n",
    "    # will be kept. The value 240 is a good starting point but can be tuned.\n",
    "    # It effectively removes very light/white background areas.\n",
    "    _, mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Use the mask to black out the background.\n",
    "    segmented_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    return segmented_image\n",
    "\n",
    "def apply_clahe(image):\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab_image)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    merged_channels = cv2.merge([cl, a, b])\n",
    "    return cv2.cvtColor(merged_channels, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "def apply_gaussian_blur(image):\n",
    "    return cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "def apply_median_blur(image):\n",
    "    return cv2.medianBlur(image, 5)\n",
    "\n",
    "def apply_wiener_filter(image):\n",
    "    img_float = image.astype(np.float64) / 255.0\n",
    "    filtered_channels = [wiener(channel) for channel in cv2.split(img_float)]\n",
    "    filtered_image = cv2.merge(filtered_channels)\n",
    "    return (np.clip(filtered_image, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "def apply_histogram_equalization(image):\n",
    "    img_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "    img_ycrcb[:, :, 0] = cv2.equalizeHist(img_ycrcb[:, :, 0])\n",
    "    return cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\n",
    "\n",
    "def apply_laplacian_filter(image):\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    abs_laplacian = np.absolute(laplacian)\n",
    "    return np.uint8(abs_laplacian)\n",
    "\n",
    "def apply_average_filter(image):\n",
    "    return cv2.blur(image, (5, 5))\n",
    "\n",
    "def apply_sobel_filter(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n",
    "    sobel_norm = cv2.normalize(sobel_combined, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    sobel_uint8 = np.uint8(sobel_norm)\n",
    "    return cv2.cvtColor(sobel_uint8, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def apply_canny_filter(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, threshold1=100, threshold2=200)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def get_preprocessing_function(model_specific_preprocess_input, is_training=False):\n",
    "    \"\"\"\n",
    "    Creates a master preprocessing function that applies a pipeline of filters\n",
    "    and augmentations before applying model-specific scaling.\n",
    "    \"\"\"\n",
    "    def master_preprocessor(image):\n",
    "        # Start with a copy to ensure original data isn't altered unexpectedly.\n",
    "        # Convert to uint8 for OpenCV compatibility.\n",
    "        processed_image = image.astype('uint8')\n",
    "\n",
    "        # --- Universal Pre-processing Pipeline ---\n",
    "        # These filters are applied to all images for all models.\n",
    "        if APPLY_STAIN_NORMALIZATION:\n",
    "            processed_image = apply_stain_normalization(processed_image)\n",
    "        if APPLY_HAIR_REMOVAL:\n",
    "            processed_image = apply_hair_removal(processed_image)\n",
    "        if APPLY_BILATERAL_FILTER:\n",
    "            processed_image = apply_bilateral_filter(processed_image)\n",
    "        if APPLY_SEGMENTATION:\n",
    "            processed_image = apply_segmentation(processed_image)\n",
    "        if APPLY_WIENER_FILTER:\n",
    "            processed_image = apply_wiener_filter(processed_image)\n",
    "        if APPLY_MEDIAN_FILTER:\n",
    "            processed_image = apply_median_blur(processed_image)\n",
    "        if APPLY_AVERAGE_FILTER:\n",
    "            processed_image = apply_average_filter(processed_image)\n",
    "        if APPLY_GAUSSIAN_BLUR:\n",
    "            processed_image = apply_gaussian_blur(processed_image)\n",
    "        if APPLY_CLAHE:\n",
    "            processed_image = apply_clahe(processed_image)\n",
    "        if APPLY_HISTOGRAM_EQUALIZATION:\n",
    "            processed_image = apply_histogram_equalization(processed_image)\n",
    "        if APPLY_LAPLACIAN_FILTER:\n",
    "            processed_image = apply_laplacian_filter(processed_image)\n",
    "        if APPLY_SOBEL_FILTER:\n",
    "            processed_image = apply_sobel_filter(processed_image)\n",
    "        if APPLY_CANNY_FILTER:\n",
    "            processed_image = apply_canny_filter(processed_image)\n",
    "\n",
    "        # Convert to float32 for augmentation and model input\n",
    "        processed_image = processed_image.astype('float32')\n",
    "\n",
    "        # --- Training-Only Augmentation ---\n",
    "        if is_training and APPLY_RANDOM_ERASING:\n",
    "            processed_image = random_erasing(processed_image)\n",
    "\n",
    "        # --- Final Model-Specific Processing ---\n",
    "        # This is the last step. It applies the specific function for pre-trained\n",
    "        # models or a simple rescale for the CustomCNN.\n",
    "        if model_specific_preprocess_input:\n",
    "            return model_specific_preprocess_input(processed_image)\n",
    "        else:\n",
    "            return processed_image / 255.0\n",
    "            \n",
    "    return master_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Function to Create the Custom CNN Model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.101488Z",
     "iopub.status.busy": "2025-08-05T11:03:40.101197Z",
     "iopub.status.idle": "2025-08-05T11:03:40.118887Z",
     "shell.execute_reply": "2025-08-05T11:03:40.118159Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.101463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_custom_cnn(input_shape, num_output_units, last_layer_activation):\n",
    "    \"\"\"Builds and returns the user-defined custom CNN model.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(16, (1, 1), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(3, 3),\n",
    "        layers.Conv2D(32, (1, 1), activation='relu'),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(3, 3),\n",
    "        layers.Conv2D(64, (1, 1), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(3, 3),\n",
    "        layers.Conv2D(16, (1, 1), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        # Add a feature extractor layer name for Grad-CAM and feature projection\n",
    "        layers.GlobalAveragePooling2D(name='feature_extractor_layer'),\n",
    "        layers.Dense(num_output_units, activation=last_layer_activation)\n",
    "    ])\n",
    "    print(\"--- Custom CNN Model Summary ---\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 3. VISUALIZATION AND REPORTING FUNCTIONS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T11:03:40.119952Z",
     "iopub.status.busy": "2025-08-05T11:03:40.119717Z",
     "iopub.status.idle": "2025-08-05T11:03:40.150437Z",
     "shell.execute_reply": "2025-08-05T11:03:40.149772Z",
     "shell.execute_reply.started": "2025-08-05T11:03:40.119926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, save_dir):\n",
    "    \"\"\"Plots accuracy, loss, and precision from the model's history.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    metrics = ['accuracy', 'loss', 'precision']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        val_metric = f'val_{metric}'\n",
    "        axes[i].plot(history.history[metric], label=f'Train {metric.capitalize()}')\n",
    "        axes[i].plot(history.history[val_metric], label=f'Validation {metric.capitalize()}')\n",
    "        axes[i].set_title(f'{model_name} - {metric.capitalize()}')\n",
    "        axes[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_training_history.png'))\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, model_name, save_dir):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_pr_curves(y_true, y_pred_prob, class_names, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plots ROC and Precision-Recall curves for binary AND multiclass classification.\n",
    "    For multiclass, it uses the One-vs-Rest (OvR) strategy.\n",
    "    \"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # --- ROC Curve (Binary and Multiclass) ---\n",
    "    if n_classes == 2:\n",
    "        # Standard binary case\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    else:\n",
    "        # Multiclass case (One-vs-Rest)\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot each class's ROC curve\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            axes[0].plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                         label=f'ROC curve of {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title(f'{model_name} - Receiver Operating Characteristic')\n",
    "    axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "    # --- Precision-Recall Curve (Binary and Multiclass) ---\n",
    "    if n_classes == 2:\n",
    "        # Standard binary case\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        axes[1].plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n",
    "    else:\n",
    "        # Multiclass case (One-vs-Rest)\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        pr_auc = dict()\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_prob[:, i])\n",
    "            pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "        # Plot each class's PR curve\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            axes[1].plot(recall[i], precision[i], color=color, lw=2,\n",
    "                         label=f'PR curve of {class_names[i]} (AP = {pr_auc[i]:.2f})')\n",
    "\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title(f'{model_name} - Precision-Recall Curve')\n",
    "    axes[1].legend(loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_roc_pr_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "def plot_projections(features, labels, class_names, model_name, save_dir):\n",
    "    \"\"\"Plots t-SNE and UMAP projections of features.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1)).fit_transform(features)\n",
    "    df_tsne = pd.DataFrame({'x': tsne[:, 0], 'y': tsne[:, 1], 'label': [class_names[l] for l in labels]})\n",
    "    sns.scatterplot(data=df_tsne, x='x', y='y', hue='label', ax=axes[0], palette='viridis').set_title(f'{model_name} - t-SNE')\n",
    "    \n",
    "    umap_proj = UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(features)\n",
    "    df_umap = pd.DataFrame({'x': umap_proj[:, 0], 'y': umap_proj[:, 1], 'label': [class_names[l] for l in labels]})\n",
    "    sns.scatterplot(data=df_umap, x='x', y='y', hue='label', ax=axes[1], palette='viridis').set_title(f'{model_name} - UMAP')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_projections.png'))\n",
    "    plt.show()\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
    "    \"\"\"Creates a Grad-CAM heatmap.\"\"\"\n",
    "    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        # For binary with sigmoid, the class channel is the output itself.\n",
    "        class_channel = preds\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    heatmap = last_conv_layer_output[0] @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path, alpha=0.4):\n",
    "    \"\"\"Saves a superimposed Grad-CAM image.\"\"\"\n",
    "    img = cv2.imread(img_path); img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])); heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = np.clip(heatmap * alpha + img, 0, 255).astype('uint8')\n",
    "    cv2.imwrite(cam_path, superimposed_img)\n",
    "\n",
    "def visualize_class_maps(model, last_conv_layer_name, preprocessor, model_name, save_dir, test_dir_path):\n",
    "    \"\"\"Displays Grad-CAM for one sample from each class in a 1xN layout.\"\"\"\n",
    "    class_names = sorted(os.listdir(test_dir_path))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        img_path = os.path.join(test_dir_path, class_name, os.listdir(os.path.join(test_dir_path, class_name))[0])\n",
    "        img_array = img_to_array(load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH)))\n",
    "        img_preprocessed = preprocessor(img_array.copy()) if preprocessor else img_array / 255.0\n",
    "        img_for_model = np.expand_dims(img_preprocessed, axis=0)\n",
    "        heatmap = make_gradcam_heatmap(img_for_model, model, last_conv_layer_name)\n",
    "        cam_path = os.path.join(save_dir, f'{model_name}_gradcam_{class_name}.png')\n",
    "        save_and_display_gradcam(img_path, heatmap, cam_path)\n",
    "        ax = plt.subplot(1, len(class_names), i + 1)\n",
    "        ax.imshow(cv2.cvtColor(cv2.imread(cam_path), cv2.COLOR_BGR2RGB)); ax.set_title(f'Grad-CAM: {class_name}'); ax.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, save_dir, num_examples_per_class=2):\n",
    "    \"\"\"Shows sample predictions, highlighting correct and incorrect ones.\"\"\"\n",
    "    filenames = test_generator.filenames\n",
    "    examples_shown = {name: 0 for name in class_names}\n",
    "    fig, axes = plt.subplots(nrows=num_examples_per_class, ncols=len(class_names), figsize=(18, 5 * num_examples_per_class), squeeze=False)\n",
    "    fig.suptitle(f'{model_name} - Prediction Samples', fontsize=20)\n",
    "    for i in range(len(filenames)):\n",
    "        if all(v >= num_examples_per_class for v in examples_shown.values()): break\n",
    "        true_label_idx = y_true[i]\n",
    "        true_label_name = class_names[true_label_idx]\n",
    "        if examples_shown[true_label_name] < num_examples_per_class:\n",
    "            img_path = os.path.join(test_generator.directory, filenames[i])\n",
    "            ax = axes[examples_shown[true_label_name], true_label_idx]\n",
    "            ax.imshow(load_img(img_path))\n",
    "            ax.axis('off')\n",
    "            title_color = 'green' if y_pred[i] == true_label_idx else 'red'\n",
    "            ax.set_title(f\"True: {true_label_name}\\nPred: {class_names[y_pred[i]]}\", color=title_color)\n",
    "            examples_shown[true_label_name] += 1\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(os.path.join(save_dir, f'{model_name}_prediction_samples.png')); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 4. MAIN TRAINING & EVALUATION LOOP ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # --- Evaluation & Visualization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-05T11:07:14.989Z",
     "iopub.execute_input": "2025-08-05T11:03:40.151472Z",
     "iopub.status.busy": "2025-08-05T11:03:40.151164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset will be created at: /kaggle/working/cervix_cancer_dataset\n",
      "\n",
      "Processing: 'cervix_dyk' -> 'Dyskeratosis'\n",
      "  - Found 4000 images in 'train' directory.\n",
      "  - Found 1000 images in 'val' directory.\n",
      "\n",
      "Processing: 'cervix_koc' -> 'Koilocytes'\n",
      "  - Found 4000 images in 'train' directory.\n",
      "  - Found 1000 images in 'val' directory.\n",
      "\n",
      "Processing: 'cervix_mep' -> 'Metaplastic_cells'\n",
      "  - Found 4000 images in 'train' directory.\n",
      "  - Found 1000 images in 'val' directory.\n",
      "\n",
      "Processing: 'cervix_pab' -> 'Parabasal_cells'\n",
      "  - Found 4000 images in 'train' directory.\n",
      "  - Found 1000 images in 'val' directory.\n",
      "\n",
      "Processing: 'cervix_sfi' -> 'Superficial_Intermediate_cells'\n",
      "  - Found 4000 images in 'train' directory.\n",
      "  - Found 1000 images in 'val' directory.\n",
      "\n",
      "Dataset setup and file copying complete!\n",
      "\n",
      "--- Setting up training for 5 classes ---\n",
      "Class Mode: categorical, Loss: categorical_crossentropy, Activation: softmax\n",
      "\n",
      "========================= Training and Evaluating: CustomCNN =========================\n",
      "Found 17500 images belonging to 5 classes.\n",
      "Found 3750 images belonging to 5 classes.\n",
      "Found 3750 images belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754392014.178994      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Custom CNN Model Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">102,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ feature_extractor_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">85</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m64\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m102,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m1,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m6,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ feature_extractor_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m85\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,805</span> (479.71 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m122,805\u001b[0m (479.71 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,389</span> (478.08 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,389\u001b[0m (478.08 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">416</span> (1.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m416\u001b[0m (1.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1754392022.174158     119 service.cc:148] XLA service 0x7f5f50003820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1754392022.175639     119 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1754392022.829116     119 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/547\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 49ms/step - accuracy: 0.1267 - loss: 1.8409 - precision: 0.1865   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754392027.202190     119 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/547\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 64ms/step - accuracy: 0.4920 - loss: 1.2682 - precision: 0.6711"
     ]
    }
   ],
   "source": [
    "# --- Run the Setup ---\n",
    "setup_directories()\n",
    "\n",
    "# You can now use these variables to point to your new dataset\n",
    "train_dir = os.path.join(BASE_DIR, 'train')\n",
    "val_dir = os.path.join(BASE_DIR, 'val')\n",
    "test_dir = os.path.join(BASE_DIR, 'test')\n",
    "\n",
    "NUM_CLASSES = len(os.listdir(train_dir))\n",
    "\n",
    "if APPLY_GAN_AUGMENTATION and NUM_CLASSES >= 2:\n",
    "    for class_name in CLASSES_TO_AUGMENT:\n",
    "        if class_name in TARGET_CLASSES:\n",
    "            train_gan_and_generate_images(class_name, NUM_IMAGES_TO_GENERATE, train_dir)\n",
    "        else:\n",
    "            print(f\"WARNING: Class '{class_name}' for GAN is not in TARGET_CLASSES. Skipping.\")\n",
    "\n",
    "\n",
    "# --- Model Registry ---\n",
    "MODELS = {\n",
    "    'CustomCNN': (None, None),\n",
    "    'VGG16': (VGG16, tf.keras.applications.vgg16.preprocess_input),\n",
    "    'VGG19': (VGG19, tf.keras.applications.vgg19.preprocess_input),\n",
    "    'ResNet50': (ResNet50, tf.keras.applications.resnet50.preprocess_input),\n",
    "    'InceptionV3': (InceptionV3, tf.keras.applications.inception_v3.preprocess_input),\n",
    "    'Xception': (Xception, tf.keras.applications.xception.preprocess_input),\n",
    "    'MobileNetV2': (MobileNetV2, tf.keras.applications.mobilenet_v2.preprocess_input),\n",
    "    'DenseNet121': (DenseNet121, tf.keras.applications.densenet.preprocess_input),\n",
    "    'EfficientNetB0': (EfficientNetB0, tf.keras.applications.efficientnet.preprocess_input),\n",
    "    'InceptionResNetV2': (InceptionResNetV2, tf.keras.applications.inception_resnet_v2.preprocess_input),\n",
    "}\n",
    "\n",
    "\n",
    "# --- Main Training and Evaluation Loop ---\n",
    "if NUM_CLASSES >= 2:\n",
    "    # --- DYNAMIC CONFIGURATION BASED ON CLASS COUNT ---\n",
    "    if NUM_CLASSES == 2:\n",
    "        class_mode = 'binary'\n",
    "        loss_function = 'binary_crossentropy'\n",
    "        last_layer_activation = 'sigmoid'\n",
    "        num_output_units = 1\n",
    "    else:  # Multi-class\n",
    "        class_mode = 'categorical'\n",
    "        loss_function = 'categorical_crossentropy'\n",
    "        last_layer_activation = 'softmax'\n",
    "        num_output_units = NUM_CLASSES\n",
    "\n",
    "    print(f\"\\n--- Setting up training for {NUM_CLASSES} classes ---\")\n",
    "    print(f\"Class Mode: {class_mode}, Loss: {loss_function}, Activation: {last_layer_activation}\")\n",
    "\n",
    "    for model_name, (model_constructor, preprocess_input) in MODELS.items():\n",
    "        print(f\"\\n{'='*25} Training and Evaluating: {model_name} {'='*25}\")\n",
    "        model_save_dir = os.path.join(OUTPUT_DIR, model_name)\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "        # ******************************************************************\n",
    "        # --- SECTION CHANGED: Corrected Data Generators ---\n",
    "        # This implementation now correctly uses your custom get_preprocessing_function.\n",
    "        print(f\"Instantiating preprocessor for {model_name}...\")\n",
    "        \n",
    "        # For CustomCNN, pass `None`. get_preprocessing_function handles this.\n",
    "        model_specific_preprocessing = None if model_name == 'CustomCNN' else preprocess_input\n",
    "\n",
    "        # Create the master preprocessor for training data (with augmentations)\n",
    "        train_preprocessor = get_preprocessing_function(\n",
    "            model_specific_preprocess_input=model_specific_preprocessing,\n",
    "            is_training=True  # Enables training-only augmentations\n",
    "        )\n",
    "\n",
    "        # Create the master preprocessor for validation/test data (no augmentations)\n",
    "        val_test_preprocessor = get_preprocessing_function(\n",
    "            model_specific_preprocess_input=model_specific_preprocessing,\n",
    "            is_training=False # Disables augmentations for consistent evaluation\n",
    "        )\n",
    "\n",
    "        # Use these master preprocessors in your ImageDataGenerator\n",
    "        train_datagen = ImageDataGenerator(preprocessing_function=train_preprocessor)\n",
    "        val_test_datagen = ImageDataGenerator(preprocessing_function=val_test_preprocessor)\n",
    "        # ******************************************************************\n",
    "\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "            batch_size=BATCH_SIZE, class_mode=class_mode\n",
    "        )\n",
    "        validation_generator = val_test_datagen.flow_from_directory(\n",
    "            val_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "            batch_size=BATCH_SIZE, class_mode=class_mode\n",
    "        )\n",
    "        test_generator = val_test_datagen.flow_from_directory(\n",
    "            test_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "            batch_size=BATCH_SIZE, class_mode=class_mode, shuffle=False\n",
    "        )\n",
    "\n",
    "        # --- Model Building ---\n",
    "        if model_name == 'CustomCNN':\n",
    "            model = create_custom_cnn((IMG_HEIGHT, IMG_WIDTH, 3), num_output_units, last_layer_activation)\n",
    "        else:\n",
    "            base_model = model_constructor(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "            base_model.trainable = False\n",
    "            x = GlobalAveragePooling2D(name='feature_extractor_layer')(base_model.output)\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "            predictions = Dense(num_output_units, activation=last_layer_activation)(x)\n",
    "            model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        # --- Model Training ---\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=loss_function, metrics=['accuracy', Precision(name='precision')])\n",
    "        best_model_path = os.path.join(model_save_dir, f'{model_name}_best.keras')\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_accuracy', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True),\n",
    "            ModelCheckpoint(filepath=best_model_path, save_best_only=True, monitor='val_accuracy')\n",
    "        ]\n",
    "        history = model.fit(train_generator, epochs=EPOCHS, validation_data=validation_generator, callbacks=callbacks)\n",
    "\n",
    "        # --- Evaluation & Visualization ---\n",
    "        print(f\"\\n--- Loading best model from '{best_model_path}' for evaluation ---\")\n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        plot_training_history(history, model_name, model_save_dir)\n",
    "\n",
    "        y_pred_prob = model.predict(test_generator)\n",
    "        y_true = test_generator.classes\n",
    "        class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "        if class_mode == 'binary':\n",
    "            y_pred = (y_pred_prob.flatten() > 0.5).astype(int)\n",
    "        else: # categorical\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "        print(f'\\nClassification Report for {model_name}:\\n')\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names, digits=2))\n",
    "\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "        print(f\"Overall Matthew's Correlation Coefficient (MCC): {mcc:.2f}\\n\")\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "        metrics_data = []\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            TP = cm[i, i]\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            TN = cm.sum() - (TP + FP + FN)\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "            \n",
    "            metrics_data.append({\n",
    "                \"Class\": class_name,\n",
    "                \"Precision\": report_dict[class_name]['precision'],\n",
    "                \"Recall (Sensitivity)\": report_dict[class_name]['recall'],\n",
    "                \"F1-Score\": report_dict[class_name]['f1-score'],\n",
    "                \"Specificity\": specificity\n",
    "            })\n",
    "\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        print(\"Detailed Per-Class Metrics Summary:\")\n",
    "        print(metrics_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "        avg_specificity = metrics_df['Specificity'].mean()\n",
    "        print(f\"\\nAverage Specificity: {avg_specificity:.4f}\")\n",
    "\n",
    "        plot_confusion_matrix(y_true, y_pred, class_names, model_name, model_save_dir)\n",
    "\n",
    "        y_pred_for_curves = y_pred_prob.flatten() if class_mode == 'binary' else y_pred_prob\n",
    "        plot_roc_pr_curves(y_true, y_pred_for_curves, class_names, model_name, model_save_dir)\n",
    "\n",
    "        feature_extractor = Model(inputs=model.inputs, outputs=model.get_layer('feature_extractor_layer').output)\n",
    "        test_features = feature_extractor.predict(test_generator)\n",
    "        plot_projections(test_features, y_true, class_names, model_name, model_save_dir)\n",
    "\n",
    "        last_conv_layer_name = next((layer.name for layer in reversed(model.layers) if 'conv' in layer.name.lower()), None)\n",
    "        # For Grad-CAM, pass the raw preprocessor for the model, not the one with augmentations\n",
    "        grad_cam_preprocessor = get_preprocessing_function(model_specific_preprocessing, is_training=False)\n",
    "        if last_conv_layer_name and model_name != 'CustomCNN':\n",
    "            print(f\"Generating Grad-CAM for {model_name} using layer: {last_conv_layer_name}\")\n",
    "            visualize_class_maps(model, last_conv_layer_name, grad_cam_preprocessor, model_name, model_save_dir, test_dir)\n",
    "        else:\n",
    "            print(f\"Skipping Grad-CAM visualization for {model_name}.\")\n",
    "\n",
    "        visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, model_save_dir)\n",
    "        print(f\"\\nFinished processing {model_name}. Results saved to {model_save_dir}\")\n",
    "\n",
    "    print(\"\\nAll models have been trained and evaluated.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping model training because a valid dataset with 2 or more classes was not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7857386,
     "sourceId": 12456074,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
