{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12616952,"sourceType":"datasetVersion","datasetId":7971034}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom PIL import Image\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom scipy.signal import wiener\n\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.applications import (\n    VGG16, VGG19, ResNet50, InceptionV3, Xception, MobileNetV2, \n    DenseNet121, EfficientNetB0, InceptionResNetV2\n)\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.metrics import Precision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 1. CONFIGURATION AND SETUP ---","metadata":{}},{"cell_type":"markdown","source":"# --- Dataset and Output Paths ---","metadata":{}},{"cell_type":"code","source":"SOURCE_DIR = '/kaggle/input/fracatlas/FracAtlas/images'\nBASE_DIR = '/kaggle/working/bone_fracture_dataset_final'\nOUTPUT_DIR = '/kaggle/working/model_outputs'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Model and Training Parameters ---","metadata":{}},{"cell_type":"code","source":"IMG_HEIGHT = 224\nIMG_WIDTH = 224\nBATCH_SIZE = 32\nNUM_CLASSES = 1 \nEPOCHS = 25\nEARLY_STOPPING_PATIENCE = 10\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Preprocessing Toggles ---","metadata":{}},{"cell_type":"code","source":"APPLY_SEGMENTATION = False\nAPPLY_CLAHE = False\nAPPLY_GAUSSIAN_BLUR = False \nAPPLY_MEDIAN_FILTER = False\nAPPLY_WIENER_FILTER = False\nAPPLY_HISTOGRAM_EQUALIZATION = False\nAPPLY_LAPLACIAN_FILTER = False\nAPPLY_AVERAGE_FILTER = False\nAPPLY_SOBEL_FILTER = False\nAPPLY_CANNY_FILTER = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_directories():\n    \n    if os.path.exists(BASE_DIR):\n        print(f\"Directory '{BASE_DIR}' already exists. Deleting it to recreate...\")\n        shutil.rmtree(BASE_DIR)\n    \n    print(\"Creating new train/val/test directories...\")\n    train_dir = os.path.join(BASE_DIR, 'train')\n    val_dir = os.path.join(BASE_DIR, 'val')\n    test_dir = os.path.join(BASE_DIR, 'test')\n    \n    os.makedirs(train_dir)\n    os.makedirs(val_dir)\n    os.makedirs(test_dir)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    try:\n        class_names = [\n            d for d in os.listdir(SOURCE_DIR)\n            if os.path.isdir(os.path.join(SOURCE_DIR, d))\n        ]\n        \n        print(class_names)\n        print(f\"Found {len(class_names)} matching class directories to process.\")\n\n        for class_name in class_names:\n            print(f\"Processing '{class_name}'...\")\n            \n            for d in [train_dir, val_dir, test_dir]:\n                os.makedirs(os.path.join(d, class_name), exist_ok=True)\n            \n            src_path = os.path.join(SOURCE_DIR, class_name)\n            all_files = [f for f in os.listdir(src_path) if os.path.isfile(os.path.join(src_path, f))]\n\n            # --- Start: New code to verify images and filter out corrupted ones --- a_id=2\n            valid_files = []\n            print(f\"Verifying {len(all_files)} images in '{class_name}'...\")\n            for f in all_files:\n                # Process only common image file types\n                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                    image_path = os.path.join(src_path, f)\n                    try:\n                        # Attempt to open and load the image\n                        img = Image.open(image_path)\n                        img.load() \n                        # If successful, add to the list of valid files\n                        valid_files.append(f)\n                    except (IOError, OSError) as e:\n                        # If an error occurs, the image is likely corrupted\n                        print(f\"  - Corrupted image skipped: {f} (Reason: {e})\")\n                else:\n                    print(f\"  - Skipped non-image file: {f}\")\n\n            print(f\"Found {len(valid_files)} valid images in '{class_name}'.\")\n            # --- End: New code --- a_id=3\n\n            # Shuffle and split the list of *valid* files\n            np.random.shuffle(valid_files)\n            \n            train_split, val_split = 0.7, 0.15\n            train_end = int(len(valid_files) * train_split)\n            val_end = train_end + int(len(valid_files) * val_split)\n            # Use the filtered 'valid_files' list for splitting\n            train_files, val_files, test_files = valid_files[:train_end], valid_files[train_end:val_end], valid_files[val_end:]\n            \n            # Copy valid files into the correctly named directories\n            for f in train_files:\n                shutil.copy(os.path.join(src_path, f), os.path.join(train_dir, class_name, f))\n            for f in val_files:\n                shutil.copy(os.path.join(src_path, f), os.path.join(val_dir, class_name, f))\n            for f in test_files:\n                shutil.copy(os.path.join(src_path, f), os.path.join(test_dir, class_name, f))\n\n        print(\"\\nData splitting and directory setup complete.\")\n        print(f\"Dataset created at: '{BASE_DIR}'\")\n\n    except FileNotFoundError:\n        print(f\"ERROR: Source directory not found at '{SOURCE_DIR}'. Please check the path.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n# --- Run the Setup ---\nsetup_directories()\n\n# You can now use these variables to point to your new dataset\ntrain_dir = os.path.join(BASE_DIR, 'train')\nval_dir = os.path.join(BASE_DIR, 'val')\ntest_dir = os.path.join(BASE_DIR, 'test')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 2. PREPROCESSING FUNCTIONS ---","metadata":{}},{"cell_type":"code","source":"def apply_segmentation(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)    \n    _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    segmented_image = cv2.bitwise_and(image, image, mask=mask)\n    return segmented_image\n\ndef apply_clahe(image):\n    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab_image)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    merged_channels = cv2.merge([cl, a, b])\n    return cv2.cvtColor(merged_channels, cv2.COLOR_LAB2RGB)\n\ndef apply_gaussian_blur(image):\n    return cv2.GaussianBlur(image, (5, 5), 0)\n\ndef apply_median_blur(image):\n    return cv2.medianBlur(image, 5)\n\ndef apply_wiener_filter(image):\n    img_float = image.astype(np.float64) / 255.0\n    filtered_channels = [wiener(channel) for channel in cv2.split(img_float)]\n    filtered_image = cv2.merge(filtered_channels)\n    return (np.clip(filtered_image, 0, 1) * 255).astype(np.uint8)\n\ndef apply_histogram_equalization(image):\n    img_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n    img_ycrcb[:, :, 0] = cv2.equalizeHist(img_ycrcb[:, :, 0])\n    return cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\n\ndef apply_laplacian_filter(image):\n    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n    abs_laplacian = np.absolute(laplacian)\n    return np.uint8(abs_laplacian)\n\ndef apply_average_filter(image):\n    return cv2.blur(image, (5, 5))\n\ndef apply_sobel_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n    sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n    sobel_norm = cv2.normalize(sobel_combined, None, 0, 255, cv2.NORM_MINMAX)\n    sobel_uint8 = np.uint8(sobel_norm)\n    return cv2.cvtColor(sobel_uint8, cv2.COLOR_GRAY2RGB)\n\ndef apply_canny_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    edges = cv2.Canny(gray, threshold1=100, threshold2=200)\n    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n\ndef get_preprocessing_function(model_specific_preprocess_input, is_training=False):\n    def master_preprocessing_function(image):\n        if model_specific_preprocess_input is None:\n            return image / 255.0\n            \n        processed_image = image.astype('uint8')\n        if APPLY_SEGMENTATION: processed_image = apply_segmentation(processed_image)\n        if APPLY_WIENER_FILTER: processed_image = apply_wiener_filter(processed_image)\n        if APPLY_MEDIAN_FILTER: processed_image = apply_median_blur(processed_image)\n        if APPLY_AVERAGE_FILTER: processed_image = apply_average_filter(processed_image)\n        if APPLY_GAUSSIAN_BLUR: processed_image = apply_gaussian_blur(processed_image)\n        if APPLY_CLAHE: processed_image = apply_clahe(processed_image)\n        if APPLY_HISTOGRAM_EQUALIZATION: processed_image = apply_histogram_equalization(processed_image)\n        if APPLY_LAPLACIAN_FILTER: processed_image = apply_laplacian_filter(processed_image)\n        if APPLY_SOBEL_FILTER: processed_image = apply_sobel_filter(processed_image)\n        if APPLY_CANNY_FILTER: processed_image = apply_canny_filter(processed_image)\n        processed_image = processed_image.astype('float32')\n        return model_specific_preprocess_input(processed_image)\n    return master_preprocessing_function","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Function to Create the Custom CNN Model ---","metadata":{}},{"cell_type":"code","source":"def create_custom_cnn(input_shape, num_classes):\n    \"\"\"Builds and returns the user-defined custom CNN model.\"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),\n        layers.Conv2D(16, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(32, (1, 1), activation='relu'),\n        layers.Conv2D(32, (3, 3), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(64, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (5, 5), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(16, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(16, (5, 5), activation='relu'),\n        layers.BatchNormalization(),\n        # Add a feature extractor layer name for Grad-CAM and feature projection\n        layers.GlobalAveragePooling2D(name='feature_extractor_layer'),\n        layers.Dense(num_classes, activation='sigmoid')\n    ])\n    print(\"--- Custom CNN Model Summary ---\")\n    model.summary()\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 3. VISUALIZATION AND REPORTING FUNCTIONS ---","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history, model_name, save_dir):\n    \"\"\"Plots accuracy, loss, and precision from the model's history.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n    metrics = ['accuracy', 'loss', 'precision']\n    for i, metric in enumerate(metrics):\n        val_metric = f'val_{metric}'\n        axes[i].plot(history.history[metric], label=f'Train {metric.capitalize()}')\n        axes[i].plot(history.history[val_metric], label=f'Validation {metric.capitalize()}')\n        axes[i].set_title(f'{model_name} - {metric.capitalize()}')\n        axes[i].legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_training_history.png'))\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, model_name, save_dir):\n    \"\"\"Plots a confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'))\n    plt.show()\n\ndef plot_roc_pr_curves(y_true, y_pred_prob, model_name, save_dir):\n    \"\"\"Plots ROC and Precision-Recall curves, simplified for binary classification.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    axes[0].set_title(f'{model_name} - ROC Curve'); axes[0].legend(loc=\"lower right\")\n\n    # Precision-Recall Curve\n    precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n    pr_auc = auc(recall, precision)\n    axes[1].plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n    axes[1].set_title(f'{model_name} - Precision-Recall Curve'); axes[1].legend(loc=\"lower left\")\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_roc_pr_curves.png'))\n    plt.show()\n\ndef plot_projections(features, labels, class_names, model_name, save_dir):\n    \"\"\"Plots t-SNE and UMAP projections of features.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n    \n    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1)).fit_transform(features)\n    df_tsne = pd.DataFrame({'x': tsne[:, 0], 'y': tsne[:, 1], 'label': [class_names[l] for l in labels]})\n    sns.scatterplot(data=df_tsne, x='x', y='y', hue='label', ax=axes[0], palette='viridis').set_title(f'{model_name} - t-SNE')\n    \n    umap_proj = UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(features)\n    df_umap = pd.DataFrame({'x': umap_proj[:, 0], 'y': umap_proj[:, 1], 'label': [class_names[l] for l in labels]})\n    sns.scatterplot(data=df_umap, x='x', y='y', hue='label', ax=axes[1], palette='viridis').set_title(f'{model_name} - UMAP')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_projections.png'))\n    plt.show()\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n    \"\"\"Creates a Grad-CAM heatmap.\"\"\"\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        # For binary with sigmoid, the class channel is the output itself.\n        class_channel = preds\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    heatmap = last_conv_layer_output[0] @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path, alpha=0.4):\n    \"\"\"Saves a superimposed Grad-CAM image.\"\"\"\n    img = cv2.imread(img_path); img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])); heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = np.clip(heatmap * alpha + img, 0, 255).astype('uint8')\n    cv2.imwrite(cam_path, superimposed_img)\n\ndef visualize_class_maps(model, last_conv_layer_name, preprocessor, model_name, save_dir, test_dir_path):\n    \"\"\"Displays Grad-CAM for one sample from each class in a 1xN layout.\"\"\"\n    class_names = sorted(os.listdir(test_dir_path))\n    plt.figure(figsize=(12, 6))\n    for i, class_name in enumerate(class_names):\n        img_path = os.path.join(test_dir_path, class_name, os.listdir(os.path.join(test_dir_path, class_name))[0])\n        img_array = img_to_array(load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH)))\n        img_preprocessed = preprocessor(img_array.copy()) if preprocessor else img_array / 255.0\n        img_for_model = np.expand_dims(img_preprocessed, axis=0)\n        heatmap = make_gradcam_heatmap(img_for_model, model, last_conv_layer_name)\n        cam_path = os.path.join(save_dir, f'{model_name}_gradcam_{class_name}.png')\n        save_and_display_gradcam(img_path, heatmap, cam_path)\n        ax = plt.subplot(1, len(class_names), i + 1)\n        ax.imshow(cv2.cvtColor(cv2.imread(cam_path), cv2.COLOR_BGR2RGB)); ax.set_title(f'Grad-CAM: {class_name}'); ax.axis(\"off\")\n    plt.tight_layout(); plt.show()\n\ndef visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, save_dir, num_examples_per_class=2):\n    \"\"\"Shows sample predictions, highlighting correct and incorrect ones.\"\"\"\n    filenames = test_generator.filenames\n    examples_shown = {name: 0 for name in class_names}\n    fig, axes = plt.subplots(nrows=num_examples_per_class, ncols=len(class_names), figsize=(18, 5 * num_examples_per_class), squeeze=False)\n    fig.suptitle(f'{model_name} - Prediction Samples', fontsize=20)\n    for i in range(len(filenames)):\n        if all(v >= num_examples_per_class for v in examples_shown.values()): break\n        true_label_idx = y_true[i]\n        true_label_name = class_names[true_label_idx]\n        if examples_shown[true_label_name] < num_examples_per_class:\n            img_path = os.path.join(test_generator.directory, filenames[i])\n            ax = axes[examples_shown[true_label_name], true_label_idx]\n            ax.imshow(load_img(img_path))\n            ax.axis('off')\n            title_color = 'green' if y_pred[i] == true_label_idx else 'red'\n            ax.set_title(f\"True: {true_label_name}\\nPred: {class_names[y_pred[i]]}\", color=title_color)\n            examples_shown[true_label_name] += 1\n    fig.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(os.path.join(save_dir, f'{model_name}_prediction_samples.png')); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- 4. MAIN TRAINING & EVALUATION LOOP ---","metadata":{}},{"cell_type":"markdown","source":" # --- Evaluation & Visualization ---","metadata":{}},{"cell_type":"code","source":"# --- Model Registry ---\nMODELS = {\n    'CustomCNN': (None, None),\n    'VGG16': (VGG16, tf.keras.applications.vgg16.preprocess_input),\n    'VGG19': (VGG19, tf.keras.applications.vgg19.preprocess_input),\n    'ResNet50': (ResNet50, tf.keras.applications.resnet50.preprocess_input),\n    'InceptionV3': (InceptionV3, tf.keras.applications.inception_v3.preprocess_input),\n    'Xception': (Xception, tf.keras.applications.xception.preprocess_input),\n    'MobileNetV2': (MobileNetV2, tf.keras.applications.mobilenet_v2.preprocess_input),\n    'DenseNet121': (DenseNet121, tf.keras.applications.densenet.preprocess_input),\n    'EfficientNetB0': (EfficientNetB0, tf.keras.applications.efficientnet.preprocess_input),\n    'InceptionResNetV2': (InceptionResNetV2, tf.keras.applications.inception_resnet_v2.preprocess_input),\n}\n\nclass_mode = 'binary'\nloss_function = 'binary_crossentropy'\nlast_layer_activation = 'sigmoid'\nnum_output_units = 1\n\n# Main Training and Evaluation Loop\nfor model_name, (model_constructor, preprocess_input) in MODELS.items():\n    print(f\"\\n{'='*25} Training and Evaluating: {model_name} {'='*25}\")\n    model_save_dir = os.path.join(OUTPUT_DIR, model_name)\n    os.makedirs(model_save_dir, exist_ok=True)\n    \n    # ******************************************************************\n    # --- SECTION CHANGED: Corrected Data Generators ---\n    # This implementation now correctly uses your custom get_preprocessing_function.\n    print(f\"Instantiating preprocessor for {model_name}...\")\n    \n    # For CustomCNN, pass `None`. get_preprocessing_function handles this.\n    model_specific_preprocessing = None if model_name == 'CustomCNN' else preprocess_input\n\n    # Create the master preprocessor for training data (with augmentations)\n    train_preprocessor = get_preprocessing_function(\n        model_specific_preprocess_input=model_specific_preprocessing,\n        is_training=True  # Enables training-only augmentations\n    )\n\n    # Create the master preprocessor for validation/test data (no augmentations)\n    val_test_preprocessor = get_preprocessing_function(\n        model_specific_preprocess_input=model_specific_preprocessing,\n        is_training=False # Disables augmentations for consistent evaluation\n    )\n\n    # Use these master preprocessors in your ImageDataGenerator\n    train_datagen = ImageDataGenerator(preprocessing_function=train_preprocessor, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n    val_test_datagen = ImageDataGenerator(preprocessing_function=val_test_preprocessor)\n    # ******************************************************************\n\n\n    train_generator = train_datagen.flow_from_directory(\n        train_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n        batch_size=BATCH_SIZE, class_mode=class_mode\n    )\n    validation_generator = val_test_datagen.flow_from_directory(\n        val_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n        batch_size=BATCH_SIZE, class_mode=class_mode\n    )\n    test_generator = val_test_datagen.flow_from_directory(\n        test_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n        batch_size=BATCH_SIZE, class_mode=class_mode, shuffle=False\n    )\n    \n    # Model Building\n    if model_name == 'CustomCNN':\n        model = create_custom_cnn(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n    else:\n        base_model = model_constructor(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n        base_model.trainable = False\n        x = GlobalAveragePooling2D(name='feature_extractor_layer')(base_model.output)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.5)(x)\n        predictions = Dense(NUM_CLASSES, activation='sigmoid')(x)\n        model = Model(inputs=base_model.input, outputs=predictions)\n    \n    # Model Training\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy', Precision(name='precision')])\n    best_model_path = os.path.join(model_save_dir, f'{model_name}_best.keras')\n    callbacks = [\n        EarlyStopping(monitor='val_accuracy', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True),\n        ModelCheckpoint(filepath=best_model_path, save_best_only=True, monitor='val_accuracy')\n    ]\n    history = model.fit(train_generator, epochs=EPOCHS, validation_data=validation_generator, callbacks=callbacks)\n    \n    # Evaluation & Visualization\n    print(f\"\\n--- Loading best model from '{best_model_path}' for evaluation ---\")\n    model = tf.keras.models.load_model(best_model_path)\n    \n    plot_training_history(history, model_name, model_save_dir)\n    \n    y_pred_prob = model.predict(test_generator).flatten()\n    y_pred = (y_pred_prob > 0.5).astype(int)\n    y_true = test_generator.classes\n    class_names = list(test_generator.class_indices.keys())\n    \n    print(f'\\nClassification Report for {model_name}:\\n{classification_report(y_true, y_pred, target_names=class_names)}')\n    plot_confusion_matrix(y_true, y_pred, class_names, model_name, model_save_dir)\n    plot_roc_pr_curves(y_true, y_pred_prob, model_name, model_save_dir)\n    \n    # Feature Projection Visualization\n    feature_extractor = Model(inputs=model.inputs, outputs=model.get_layer('feature_extractor_layer').output)\n    test_features = feature_extractor.predict(test_generator)\n    plot_projections(test_features, y_true, class_names, model_name, model_save_dir)\n    \n    # --- KEY CHANGE: Grad-CAM Visualization Skipped for CustomCNN ---\n    last_conv_layer_name = next((layer.name for layer in reversed(model.layers) if 'conv' in layer.name.lower()), None)\n    \n    # Only run Grad-CAM if a conv layer is found AND the model is NOT the CustomCNN\n    if last_conv_layer_name and model_name != 'CustomCNN':\n        print(f\"Generating Grad-CAM for {model_name} using layer: {last_conv_layer_name}\")\n        preprocessor = preprocess_input\n        visualize_class_maps(model, last_conv_layer_name, preprocessor, model_name, model_save_dir, test_dir)\n    else:\n        # This will now be triggered for CustomCNN or if no conv layer is found\n        print(f\"Skipping Grad-CAM visualization for {model_name}.\")\n    \n    # Prediction Visualization\n    visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, model_save_dir)\n\n    print(f\"\\nFinished processing {model_name}. Results saved to {model_save_dir}\")\n\nprint(\"\\nAll models have been trained and evaluated.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_ensemble(ensemble_model_names, ensemble_name):\n    \"\"\"\n    Loads a list of binary classification models, averages their predictions, \n    and evaluates the resulting ensemble.\n    \"\"\"\n    print(f\"\\n{'='*20} Creating and Evaluating Ensemble: {ensemble_name} {'='*20}\")\n    \n    all_preds_prob = []\n    y_true_ensemble = None\n    class_names_ensemble = None\n\n    for model_name in ensemble_model_names:\n        print(f\"--- Loading model: {model_name} for ensembling ---\")\n        \n        # Get the preprocessing function from the MODELS dictionary\n        _ , preprocess_input = MODELS.get(model_name, (None, None))\n        \n        model_path = os.path.join(OUTPUT_DIR, model_name, f'{model_name}_best.keras')\n        if not os.path.exists(model_path):\n            print(f\"Warning: Model file not found for {model_name} at {model_path}. Skipping.\")\n            continue\n        \n        model = tf.keras.models.load_model(model_path)\n        \n        # Define the appropriate data generator for the model\n        if model_name == 'CustomCNN':\n             test_datagen = ImageDataGenerator(rescale=1./255)\n        else:\n             test_datagen = ImageDataGenerator(preprocessing_function=val_test_preprocessor)\n\n        # CHANGE 1: Use `class_mode='binary'` for the generator\n        test_generator_ensemble = test_datagen.flow_from_directory(\n            test_dir, \n            target_size=(IMG_HEIGHT, IMG_WIDTH), \n            batch_size=BATCH_SIZE,\n            class_mode='binary',  # Corrected for binary classification\n            shuffle=False\n        )\n        \n        # Get the ground truth labels and class names only once\n        if y_true_ensemble is None:\n            y_true_ensemble = test_generator_ensemble.classes\n            class_names_ensemble = list(test_generator_ensemble.class_indices.keys())\n            \n        # CHANGE 2: Get predictions and flatten them for binary output\n        # model.predict() will return shape (n_samples, 1), so we flatten it\n        preds_prob = model.predict(test_generator_ensemble).flatten()\n        all_preds_prob.append(preds_prob)\n\n    if len(all_preds_prob) < 2:\n        print(f\"Could not create ensemble '{ensemble_name}' because fewer than 2 valid models were found.\")\n        return\n\n    print(f\"\\n--- Averaging predictions for '{ensemble_name}' ---\")\n    \n    # CHANGE 3: Average probabilities and determine class prediction via threshold\n    ensemble_preds_prob = np.mean(all_preds_prob, axis=0)\n    ensemble_y_pred = (ensemble_preds_prob > 0.5).astype(int) # Use 0.5 threshold\n\n    # Create a directory to save ensemble results\n    ensemble_save_dir = os.path.join(OUTPUT_DIR, ensemble_name.replace(\" \", \"_\").lower())\n    os.makedirs(ensemble_save_dir, exist_ok=True)\n\n    # --- Evaluation ---\n    print(f'\\nClassification Report for {ensemble_name}:\\n{classification_report(y_true_ensemble, ensemble_y_pred, target_names=class_names_ensemble)}')\n    \n    # Plotting functions\n    plot_confusion_matrix(y_true_ensemble, ensemble_y_pred, class_names_ensemble, ensemble_name, ensemble_save_dir)\n    \n    # CHANGE 4: Call plot_roc_pr_curves with the correct 4 arguments\n    plot_roc_pr_curves(y_true_ensemble, ensemble_preds_prob, ensemble_name, ensemble_save_dir)\n    \n    print(f\"\\nFinished processing {ensemble_name}. Results saved to {ensemble_save_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# --- Define and run hard-coded ensembles ---","metadata":{}},{"cell_type":"code","source":"# Ensemble of 2 models\nevaluate_ensemble(['VGG16', 'EfficientNetB0'], \"Ensemble of 2 (VGG16, EfficientNetB0)\")\nevaluate_ensemble(['VGG19', 'EfficientNetB0'], \"Ensemble of 2 (VGG19, EfficientNetB0)\")\nevaluate_ensemble(['ResNet50', 'EfficientNetB0'], \"Ensemble of 2 (ResNet50, EfficientNetB0)\")\nevaluate_ensemble(['MobileNetV2', 'EfficientNetB0'], \"Ensemble of 2 (MobileNetV2, EfficientNetB0)\")\nevaluate_ensemble(['ResNet50', 'MobileNetV2'], \"Ensemble of 2 (ResNet50, MobileNetV2)\")\nevaluate_ensemble(['Xception', 'MobileNetV2'], \"Ensemble of 2 (Xception, MobileNetV2)\")\nevaluate_ensemble(['EfficientNetB0', 'Xception'], \"Ensemble of 2 (EfficientNetB0, Xception)\")\nevaluate_ensemble(['DenseNet121', 'EfficientNetB0'], \"Ensemble of 2 (DenseNet121, EfficientNetB0)\")\n\n# Ensemble of 3 models\nevaluate_ensemble(['VGG19', 'EfficientNetB0', 'InceptionV3'], \"Ensemble of 3 (VGG19, EfficientNetB0, InceptionV3)\")\nevaluate_ensemble(['MobileNetV2', 'EfficientNetB0', 'InceptionV3'], \"Ensemble of 3 (MobileNetV2, EfficientNetB0, InceptionV3)\")\nevaluate_ensemble(['DenseNet121', 'EfficientNetB0', 'MobileNetV2'], \"Ensemble of 3 (DenseNet121, EfficientNetB0, MobileNetV2)\")\nevaluate_ensemble(['EfficientNetB0', 'VGG19', 'MobileNetV2'], \"Ensemble of 3 (EfficientNetB0, VGG19, MobileNetV2)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}