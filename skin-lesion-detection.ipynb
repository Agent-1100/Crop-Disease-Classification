{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12456074,"sourceType":"datasetVersion","datasetId":7857386}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:04:36.613848Z","iopub.execute_input":"2025-08-03T17:04:36.614099Z","iopub.status.idle":"2025-08-03T17:04:36.621219Z","shell.execute_reply.started":"2025-08-03T17:04:36.614077Z","shell.execute_reply":"2025-08-03T17:04:36.620501Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom PIL import Image\nfrom itertools import cycle\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom scipy.signal import wiener\n\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, roc_curve, auc,\n    precision_recall_curve, matthews_corrcoef\n)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.applications import (\n    VGG16, VGG19, ResNet50, InceptionV3, Xception, MobileNetV2, \n    DenseNet121, EfficientNetB0, InceptionResNetV2\n)\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Average\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.metrics import Precision","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:04:36.622611Z","iopub.execute_input":"2025-08-03T17:04:36.622871Z","iopub.status.idle":"2025-08-03T17:05:13.466308Z","shell.execute_reply.started":"2025-08-03T17:04:36.622847Z","shell.execute_reply":"2025-08-03T17:05:13.465530Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-08-03 17:04:39.994845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754240680.230919      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754240680.302108      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# --- 1. CONFIGURATION AND SETUP ---","metadata":{}},{"cell_type":"markdown","source":"# --- Dataset and Output Paths ---","metadata":{}},{"cell_type":"code","source":"SOURCE_DIR = '/kaggle/input/multicancer-dataset/Cancer_Dataset'\nBASE_DIR = '/kaggle/working/skin_cancer_dataset'\nOUTPUT_DIR = '/kaggle/working/model_outputs'","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.467058Z","iopub.execute_input":"2025-08-03T17:05:13.467578Z","iopub.status.idle":"2025-08-03T17:05:13.471441Z","shell.execute_reply.started":"2025-08-03T17:05:13.467560Z","shell.execute_reply":"2025-08-03T17:05:13.470654Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# --- Model and Training Parameters ---","metadata":{}},{"cell_type":"code","source":"IMG_HEIGHT = 224\nIMG_WIDTH = 224\nBATCH_SIZE = 32\nEPOCHS = 25\nEARLY_STOPPING_PATIENCE = 10\n","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.472042Z","iopub.execute_input":"2025-08-03T17:05:13.472208Z","iopub.status.idle":"2025-08-03T17:05:13.487401Z","shell.execute_reply.started":"2025-08-03T17:05:13.472195Z","shell.execute_reply":"2025-08-03T17:05:13.486858Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"TARGET_CLASSES = ['Basal Cell Carcinoma', 'Squamous Cell Carcinoma', 'Melanoma', 'Actinic Keratosis', 'Vascular Lesion']\nData_Selector = 'Skin_'\n# NEW: This will be set dynamically based on the folders found.\nNUM_CLASSES = 0 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:05:13.488992Z","iopub.execute_input":"2025-08-03T17:05:13.489192Z","iopub.status.idle":"2025-08-03T17:05:13.501955Z","shell.execute_reply.started":"2025-08-03T17:05:13.489178Z","shell.execute_reply":"2025-08-03T17:05:13.501360Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# --- Preprocessing Toggles ---","metadata":{}},{"cell_type":"code","source":"APPLY_SEGMENTATION = False\nAPPLY_CLAHE = False\nAPPLY_GAUSSIAN_BLUR = False \nAPPLY_MEDIAN_FILTER = False\nAPPLY_WIENER_FILTER = False\nAPPLY_HISTOGRAM_EQUALIZATION = False\nAPPLY_LAPLACIAN_FILTER = False\nAPPLY_AVERAGE_FILTER = False\nAPPLY_SOBEL_FILTER = False\nAPPLY_CANNY_FILTER = False\n\n# --- Advanced Pre-processing & Augmentation Flags ---\nAPPLY_HAIR_REMOVAL = False\nAPPLY_BILATERAL_FILTER = False\nAPPLY_RANDOM_ERASING = False\nAPPLY_GAN_AUGMENTATION = False # Master switch for GAN","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.502581Z","iopub.execute_input":"2025-08-03T17:05:13.502789Z","iopub.status.idle":"2025-08-03T17:05:13.516859Z","shell.execute_reply.started":"2025-08-03T17:05:13.502774Z","shell.execute_reply":"2025-08-03T17:05:13.516339Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- GAN-Specific Configuration (MODIFIED) ---\nif APPLY_GAN_AUGMENTATION:\n    CLASSES_TO_AUGMENT = ['Malignant']\n    NUM_IMAGES_TO_GENERATE = 100\n    GAN_IMG_SIZE = 224  # MODIFIED: Changed to 224 to match the classifier\n    LATENT_DIM = 100\n    GAN_EPOCHS = 75 # Increased epochs slightly for the more complex generation task","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:05:13.517495Z","iopub.execute_input":"2025-08-03T17:05:13.517681Z","iopub.status.idle":"2025-08-03T17:05:13.537753Z","shell.execute_reply.started":"2025-08-03T17:05:13.517667Z","shell.execute_reply":"2025-08-03T17:05:13.537071Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def setup_directories():\n    \"\"\"\n    Selects specific classes from TARGET_CLASSES, finds their source folders (by adding\n    a 'Skin_' prefix), and creates a new, clean-named dataset for training.\n    \"\"\"\n    global NUM_CLASSES\n    if os.path.exists(BASE_DIR):\n        shutil.rmtree(BASE_DIR)\n\n    train_dir = os.path.join(BASE_DIR, 'train')\n    val_dir = os.path.join(BASE_DIR, 'val')\n    test_dir = os.path.join(BASE_DIR, 'test')\n    os.makedirs(train_dir); os.makedirs(val_dir); os.makedirs(test_dir)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    try:\n        source_train_dir = os.path.join(SOURCE_DIR, 'train')\n        NUM_CLASSES = len(TARGET_CLASSES)\n        print(f\"Targeting {NUM_CLASSES} specified classes: {TARGET_CLASSES}\")\n\n        if NUM_CLASSES < 2:\n            print(\"ERROR: Please specify at least 2 classes in TARGET_CLASSES to proceed.\")\n            return\n\n        for class_name_short in TARGET_CLASSES:  # e.g., 'Acne'\n            print(f\"Processing '{class_name_short}'...\")\n            \n            # The full name used to find the source folder (e.g., 'Skin_Acne')\n            class_name_full = f\"Skin_{class_name_short}\"\n\n            # Create destination directories using the clean, SHORT name\n            for d in [train_dir, val_dir, test_dir]:\n                os.makedirs(os.path.join(d, class_name_short), exist_ok=True)\n\n            # Aggregate images from source using the FULL name\n            all_files = []\n            for subfolder in ['train', 'val']:\n                src_path = os.path.join(SOURCE_DIR, subfolder, class_name_full)\n                if os.path.exists(src_path):\n                    all_files.extend([os.path.join(src_path, f) for f in os.listdir(src_path)])\n                else:\n                    print(f\"  - WARNING: Source directory not found and skipped: {src_path}\")\n\n            # Verify and filter corrupted images\n            valid_files = []\n            for file_path in all_files:\n                if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n                    try:\n                        Image.open(file_path).load()\n                        valid_files.append(file_path)\n                    except (IOError, OSError):\n                        print(f\"  - Corrupted image skipped: {os.path.basename(file_path)}\")\n            \n            # Split and copy files into directories with the SHORT name\n            np.random.shuffle(valid_files)\n            train_end = int(len(valid_files) * 0.7)\n            val_end = train_end + int(len(valid_files) * 0.15)\n            train_files, val_files, test_files = valid_files[:train_end], valid_files[train_end:val_end], valid_files[val_end:]\n\n            for f_path in train_files: shutil.copy(f_path, os.path.join(train_dir, class_name_short, os.path.basename(f_path)))\n            for f_path in val_files: shutil.copy(f_path, os.path.join(val_dir, class_name_short, os.path.basename(f_path)))\n            for f_path in test_files: shutil.copy(f_path, os.path.join(test_dir, class_name_short, os.path.basename(f_path)))\n\n        print(f\"\\nData splitting and directory setup complete. Dataset created at: '{BASE_DIR}'\")\n\n    except Exception as e:\n        print(f\"An unexpected error occurred during setup: {e}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.538527Z","iopub.execute_input":"2025-08-03T17:05:13.538728Z","iopub.status.idle":"2025-08-03T17:05:13.557385Z","shell.execute_reply.started":"2025-08-03T17:05:13.538712Z","shell.execute_reply":"2025-08-03T17:05:13.556691Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# --- 2. PREPROCESSING FUNCTIONS ---","metadata":{}},{"cell_type":"code","source":"def build_generator(latent_dim):\n    \"\"\"\n    Builds the DCGAN Generator model, re-architected to output 224x224 images.\n    \"\"\"\n    model = keras.Sequential([\n        # Start with a 7x7 spatial size\n        layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(latent_dim,)),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        layers.Reshape((7, 7, 256)),\n\n        # Upsampling block 1: 7x7 -> 14x14\n        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # Upsampling block 2: 14x14 -> 28x28\n        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # Upsampling block 3: 28x28 -> 56x56\n        layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n\n        # Upsampling block 4: 56x56 -> 112x112\n        layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        \n        # Upsampling block 5: 112x112 -> 224x224\n        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'),\n        # Final output shape: (224, 224, 3)\n    ], name=\"generator\")\n    return model\n\ndef build_discriminator(input_shape):\n    \"\"\"Builds the DCGAN Discriminator model.\"\"\"\n    model = keras.Sequential([\n        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=input_shape),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        \n        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        \n        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        \n        layers.Flatten(),\n        layers.Dense(1) # Logit output\n    ], name=\"discriminator\")\n    return model\n\ndef train_gan_and_generate_images(class_name, num_to_generate, base_train_dir):\n    \"\"\"Trains a DCGAN on a specific class and saves generated images to the training folder.\"\"\"\n    print(f\"\\n--- Starting GAN Augmentation for class: {class_name} ---\")\n    target_dir = os.path.join(base_train_dir, class_name)\n    if not os.path.exists(target_dir):\n        print(f\"ERROR: Cannot find source directory for GAN training: {target_dir}\")\n        return\n\n    # 1. Load Data, resizing to the new GAN_IMG_SIZE\n    real_images = []\n    for filename in os.listdir(target_dir):\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img = load_img(os.path.join(target_dir, filename), target_size=(GAN_IMG_SIZE, GAN_IMG_SIZE))\n            real_images.append(img_to_array(img))\n    \n    if len(real_images) < BATCH_SIZE:\n        print(f\"WARNING: Not enough images ({len(real_images)}) in {target_dir} to train GAN with batch size {BATCH_SIZE}. Skipping.\")\n        return\n        \n    real_images = (np.array(real_images) - 127.5) / 127.5\n    train_dataset = tf.data.Dataset.from_tensor_slices(real_images).shuffle(len(real_images)).batch(BATCH_SIZE, drop_remainder=True)\n\n    # 2. Build Models with the correct input shape\n    generator = build_generator(LATENT_DIM)\n    discriminator = build_discriminator((GAN_IMG_SIZE, GAN_IMG_SIZE, 3))\n    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n    def discriminator_loss(real_output, fake_output):\n        return cross_entropy(tf.ones_like(real_output), real_output) + cross_entropy(tf.zeros_like(fake_output), fake_output)\n\n    def generator_loss(fake_output):\n        return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n    gen_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)\n    disc_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)\n\n    @tf.function\n    def train_step(images):\n        noise = tf.random.normal([images.shape[0], LATENT_DIM])\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            generated_images = generator(noise, training=True)\n            real_output = discriminator(images, training=True)\n            fake_output = discriminator(generated_images, training=True)\n            gen_loss = generator_loss(fake_output)\n            disc_loss = discriminator_loss(real_output, fake_output)\n        \n        grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n        grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n        disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n\n    # 3. Training Loop\n    print(f\"Training GAN for {GAN_EPOCHS} epochs on {GAN_IMG_SIZE}x{GAN_IMG_SIZE} images...\")\n    for epoch in range(GAN_EPOCHS):\n        for image_batch in train_dataset:\n            train_step(image_batch)\n        if (epoch + 1) % 10 == 0:\n            print(f\"  - GAN Epoch {epoch + 1}/{GAN_EPOCHS} completed.\")\n\n    # 4. Generate and Save Images\n    print(\"Generating synthetic images...\")\n    noise = tf.random.normal([num_to_generate, LATENT_DIM])\n    generated_images = generator(noise, training=False)\n    generated_images = (generated_images * 127.5 + 127.5).numpy().astype(np.uint8)\n    \n    for i, img_array in enumerate(generated_images):\n        img = array_to_img(img_array)\n        # REMOVED: No longer need to resize, as the GAN generates at the correct size\n        img.save(os.path.join(target_dir, f'synthetic_gan_{i+1}.png'))\n        \n    print(f\"Successfully added {num_to_generate} synthetic images to '{target_dir}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:07:31.875961Z","iopub.execute_input":"2025-08-03T17:07:31.876565Z","iopub.status.idle":"2025-08-03T17:07:31.890327Z","shell.execute_reply.started":"2025-08-03T17:07:31.876539Z","shell.execute_reply":"2025-08-03T17:07:31.889782Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def apply_hair_removal(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 17))\n    black_hat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n    _, mask = cv2.threshold(black_hat, 10, 255, cv2.THRESH_BINARY)\n    return cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)\n\ndef apply_bilateral_filter(image):\n    return cv2.bilateralFilter(image, d=9, sigmaColor=75, sigmaSpace=75)\n\ndef random_erasing(img):\n    if np.random.rand() > 0.5: return img\n    h, w, _ = img.shape\n    x = np.random.randint(0, w)\n    y = np.random.randint(0, h)\n    h_erase = int(h * np.random.uniform(0.05, 0.2))\n    w_erase = int(w * np.random.uniform(0.05, 0.2))\n    img[y:y+h_erase, x:x+w_erase] = np.random.randint(0, 255)\n    return img\n\n\ndef apply_segmentation(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)    \n    _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    segmented_image = cv2.bitwise_and(image, image, mask=mask)\n    return segmented_image\n\ndef apply_clahe(image):\n    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab_image)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    merged_channels = cv2.merge([cl, a, b])\n    return cv2.cvtColor(merged_channels, cv2.COLOR_LAB2RGB)\n\ndef apply_gaussian_blur(image):\n    return cv2.GaussianBlur(image, (5, 5), 0)\n\ndef apply_median_blur(image):\n    return cv2.medianBlur(image, 5)\n\ndef apply_wiener_filter(image):\n    img_float = image.astype(np.float64) / 255.0\n    filtered_channels = [wiener(channel) for channel in cv2.split(img_float)]\n    filtered_image = cv2.merge(filtered_channels)\n    return (np.clip(filtered_image, 0, 1) * 255).astype(np.uint8)\n\ndef apply_histogram_equalization(image):\n    img_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n    img_ycrcb[:, :, 0] = cv2.equalizeHist(img_ycrcb[:, :, 0])\n    return cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\n\ndef apply_laplacian_filter(image):\n    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n    abs_laplacian = np.absolute(laplacian)\n    return np.uint8(abs_laplacian)\n\ndef apply_average_filter(image):\n    return cv2.blur(image, (5, 5))\n\ndef apply_sobel_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n    sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n    sobel_norm = cv2.normalize(sobel_combined, None, 0, 255, cv2.NORM_MINMAX)\n    sobel_uint8 = np.uint8(sobel_norm)\n    return cv2.cvtColor(sobel_uint8, cv2.COLOR_GRAY2RGB)\n\ndef apply_canny_filter(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    edges = cv2.Canny(gray, threshold1=100, threshold2=200)\n    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n\ndef get_preprocessing_function(model_specific_preprocess_input, is_training=False):\n    \"\"\"\n    Creates a master preprocessing function that applies a pipeline of filters\n    and augmentations before applying model-specific scaling.\n    \"\"\"\n    def master_preprocessor(image):\n        # Start with a copy to ensure original data isn't altered unexpectedly.\n        # Convert to uint8 for OpenCV compatibility.\n        processed_image = image.astype('uint8')\n\n        # --- Universal Pre-processing Pipeline ---\n        # These filters are applied to all images for all models.\n        if APPLY_HAIR_REMOVAL:\n            processed_image = apply_hair_removal(processed_image)\n        if APPLY_BILATERAL_FILTER:\n            processed_image = apply_bilateral_filter(processed_image)\n        if APPLY_SEGMENTATION:\n            processed_image = apply_segmentation(processed_image)\n        if APPLY_WIENER_FILTER:\n            processed_image = apply_wiener_filter(processed_image)\n        if APPLY_MEDIAN_FILTER:\n            processed_image = apply_median_blur(processed_image)\n        if APPLY_AVERAGE_FILTER:\n            processed_image = apply_average_filter(processed_image)\n        if APPLY_GAUSSIAN_BLUR:\n            processed_image = apply_gaussian_blur(processed_image)\n        if APPLY_CLAHE:\n            processed_image = apply_clahe(processed_image)\n        if APPLY_HISTOGRAM_EQUALIZATION:\n            processed_image = apply_histogram_equalization(processed_image)\n        if APPLY_LAPLACIAN_FILTER:\n            processed_image = apply_laplacian_filter(processed_image)\n        if APPLY_SOBEL_FILTER:\n            processed_image = apply_sobel_filter(processed_image)\n        if APPLY_CANNY_FILTER:\n            processed_image = apply_canny_filter(processed_image)\n\n        # Convert to float32 for augmentation and model input\n        processed_image = processed_image.astype('float32')\n\n        # --- Training-Only Augmentation ---\n        if is_training and APPLY_RANDOM_ERASING:\n            processed_image = random_erasing(processed_image)\n\n        # --- Final Model-Specific Processing ---\n        # This is the last step. It applies the specific function for pre-trained\n        # models or a simple rescale for the CustomCNN.\n        if model_specific_preprocess_input:\n            return model_specific_preprocess_input(processed_image)\n        else:\n            return processed_image / 255.0\n            \n    return master_preprocessor","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.583273Z","iopub.execute_input":"2025-08-03T17:05:13.583473Z","iopub.status.idle":"2025-08-03T17:05:13.603646Z","shell.execute_reply.started":"2025-08-03T17:05:13.583459Z","shell.execute_reply":"2025-08-03T17:05:13.603078Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# --- Function to Create the Custom CNN Model ---","metadata":{}},{"cell_type":"code","source":"def create_custom_cnn(input_shape, num_output_units, last_layer_activation):\n    \"\"\"Builds and returns the user-defined custom CNN model.\"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),\n        layers.Conv2D(16, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(32, (1, 1), activation='relu'),\n        layers.Conv2D(32, (3, 3), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(64, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (5, 5), activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling2D(3, 3),\n        layers.Conv2D(16, (1, 1), activation='relu'),\n        layers.BatchNormalization(),\n        layers.Conv2D(16, (5, 5), activation='relu'),\n        layers.BatchNormalization(),\n        # Add a feature extractor layer name for Grad-CAM and feature projection\n        layers.GlobalAveragePooling2D(name='feature_extractor_layer'),\n        layers.Dense(num_output_units, activation=last_layer_activation)\n    ])\n    print(\"--- Custom CNN Model Summary ---\")\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.604285Z","iopub.execute_input":"2025-08-03T17:05:13.604497Z","iopub.status.idle":"2025-08-03T17:05:13.622920Z","shell.execute_reply.started":"2025-08-03T17:05:13.604482Z","shell.execute_reply":"2025-08-03T17:05:13.622431Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# --- 3. VISUALIZATION AND REPORTING FUNCTIONS ---","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history, model_name, save_dir):\n    \"\"\"Plots accuracy, loss, and precision from the model's history.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n    metrics = ['accuracy', 'loss', 'precision']\n    for i, metric in enumerate(metrics):\n        val_metric = f'val_{metric}'\n        axes[i].plot(history.history[metric], label=f'Train {metric.capitalize()}')\n        axes[i].plot(history.history[val_metric], label=f'Validation {metric.capitalize()}')\n        axes[i].set_title(f'{model_name} - {metric.capitalize()}')\n        axes[i].legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_training_history.png'))\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, model_name, save_dir):\n    \"\"\"Plots a confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'))\n    plt.show()\n\ndef plot_roc_pr_curves(y_true, y_pred_prob, class_names, model_name, save_dir):\n    \"\"\"\n    Plots ROC and Precision-Recall curves for binary AND multiclass classification.\n    For multiclass, it uses the One-vs-Rest (OvR) strategy.\n    \"\"\"\n    n_classes = len(class_names)\n    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\n    # --- ROC Curve (Binary and Multiclass) ---\n    if n_classes == 2:\n        # Standard binary case\n        fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n        roc_auc = auc(fpr, tpr)\n        axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    else:\n        # Multiclass case (One-vs-Rest)\n        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Plot each class's ROC curve\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n        for i, color in zip(range(n_classes), colors):\n            axes[0].plot(fpr[i], tpr[i], color=color, lw=2,\n                         label=f'ROC curve of {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n\n    axes[0].plot([0, 1], [0, 1], 'k--', lw=2)\n    axes[0].set_xlim([0.0, 1.0])\n    axes[0].set_ylim([0.0, 1.05])\n    axes[0].set_xlabel('False Positive Rate')\n    axes[0].set_ylabel('True Positive Rate')\n    axes[0].set_title(f'{model_name} - Receiver Operating Characteristic')\n    axes[0].legend(loc=\"lower right\")\n\n    # --- Precision-Recall Curve (Binary and Multiclass) ---\n    if n_classes == 2:\n        # Standard binary case\n        precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n        pr_auc = auc(recall, precision)\n        axes[1].plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n    else:\n        # Multiclass case (One-vs-Rest)\n        precision = dict()\n        recall = dict()\n        pr_auc = dict()\n\n        for i in range(n_classes):\n            precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_prob[:, i])\n            pr_auc[i] = auc(recall[i], precision[i])\n\n        # Plot each class's PR curve\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n        for i, color in zip(range(n_classes), colors):\n            axes[1].plot(recall[i], precision[i], color=color, lw=2,\n                         label=f'PR curve of {class_names[i]} (AP = {pr_auc[i]:.2f})')\n\n    axes[1].set_xlim([0.0, 1.0])\n    axes[1].set_ylim([0.0, 1.05])\n    axes[1].set_xlabel('Recall')\n    axes[1].set_ylabel('Precision')\n    axes[1].set_title(f'{model_name} - Precision-Recall Curve')\n    axes[1].legend(loc=\"lower left\")\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_roc_pr_curves.png'))\n    plt.show()\n\ndef plot_projections(features, labels, class_names, model_name, save_dir):\n    \"\"\"Plots t-SNE and UMAP projections of features.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n    \n    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1)).fit_transform(features)\n    df_tsne = pd.DataFrame({'x': tsne[:, 0], 'y': tsne[:, 1], 'label': [class_names[l] for l in labels]})\n    sns.scatterplot(data=df_tsne, x='x', y='y', hue='label', ax=axes[0], palette='viridis').set_title(f'{model_name} - t-SNE')\n    \n    umap_proj = UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(features)\n    df_umap = pd.DataFrame({'x': umap_proj[:, 0], 'y': umap_proj[:, 1], 'label': [class_names[l] for l in labels]})\n    sns.scatterplot(data=df_umap, x='x', y='y', hue='label', ax=axes[1], palette='viridis').set_title(f'{model_name} - UMAP')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'{model_name}_projections.png'))\n    plt.show()\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n    \"\"\"Creates a Grad-CAM heatmap.\"\"\"\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        # For binary with sigmoid, the class channel is the output itself.\n        class_channel = preds\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    heatmap = last_conv_layer_output[0] @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path, alpha=0.4):\n    \"\"\"Saves a superimposed Grad-CAM image.\"\"\"\n    img = cv2.imread(img_path); img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])); heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed_img = np.clip(heatmap * alpha + img, 0, 255).astype('uint8')\n    cv2.imwrite(cam_path, superimposed_img)\n\ndef visualize_class_maps(model, last_conv_layer_name, preprocessor, model_name, save_dir, test_dir_path):\n    \"\"\"Displays Grad-CAM for one sample from each class in a 1xN layout.\"\"\"\n    class_names = sorted(os.listdir(test_dir_path))\n    plt.figure(figsize=(12, 6))\n    for i, class_name in enumerate(class_names):\n        img_path = os.path.join(test_dir_path, class_name, os.listdir(os.path.join(test_dir_path, class_name))[0])\n        img_array = img_to_array(load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH)))\n        img_preprocessed = preprocessor(img_array.copy()) if preprocessor else img_array / 255.0\n        img_for_model = np.expand_dims(img_preprocessed, axis=0)\n        heatmap = make_gradcam_heatmap(img_for_model, model, last_conv_layer_name)\n        cam_path = os.path.join(save_dir, f'{model_name}_gradcam_{class_name}.png')\n        save_and_display_gradcam(img_path, heatmap, cam_path)\n        ax = plt.subplot(1, len(class_names), i + 1)\n        ax.imshow(cv2.cvtColor(cv2.imread(cam_path), cv2.COLOR_BGR2RGB)); ax.set_title(f'Grad-CAM: {class_name}'); ax.axis(\"off\")\n    plt.tight_layout(); plt.show()\n\ndef visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, save_dir, num_examples_per_class=2):\n    \"\"\"Shows sample predictions, highlighting correct and incorrect ones.\"\"\"\n    filenames = test_generator.filenames\n    examples_shown = {name: 0 for name in class_names}\n    fig, axes = plt.subplots(nrows=num_examples_per_class, ncols=len(class_names), figsize=(18, 5 * num_examples_per_class), squeeze=False)\n    fig.suptitle(f'{model_name} - Prediction Samples', fontsize=20)\n    for i in range(len(filenames)):\n        if all(v >= num_examples_per_class for v in examples_shown.values()): break\n        true_label_idx = y_true[i]\n        true_label_name = class_names[true_label_idx]\n        if examples_shown[true_label_name] < num_examples_per_class:\n            img_path = os.path.join(test_generator.directory, filenames[i])\n            ax = axes[examples_shown[true_label_name], true_label_idx]\n            ax.imshow(load_img(img_path))\n            ax.axis('off')\n            title_color = 'green' if y_pred[i] == true_label_idx else 'red'\n            ax.set_title(f\"True: {true_label_name}\\nPred: {class_names[y_pred[i]]}\", color=title_color)\n            examples_shown[true_label_name] += 1\n    fig.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(os.path.join(save_dir, f'{model_name}_prediction_samples.png')); plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-08-03T17:05:13.623526Z","iopub.execute_input":"2025-08-03T17:05:13.623715Z","iopub.status.idle":"2025-08-03T17:05:13.649974Z","shell.execute_reply.started":"2025-08-03T17:05:13.623701Z","shell.execute_reply":"2025-08-03T17:05:13.649389Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# --- 4. MAIN TRAINING & EVALUATION LOOP ---","metadata":{}},{"cell_type":"markdown","source":" # --- Evaluation & Visualization ---","metadata":{}},{"cell_type":"code","source":"# --- Run the Setup ---\nsetup_directories()\n\n# You can now use these variables to point to your new dataset\ntrain_dir = os.path.join(BASE_DIR, 'train')\nval_dir = os.path.join(BASE_DIR, 'val')\ntest_dir = os.path.join(BASE_DIR, 'test')\n\nNUM_CLASSES = len(os.listdir(train_dir))\n\nif APPLY_GAN_AUGMENTATION and NUM_CLASSES >= 2:\n    for class_name in CLASSES_TO_AUGMENT:\n        if class_name in TARGET_CLASSES:\n            train_gan_and_generate_images(class_name, NUM_IMAGES_TO_GENERATE, train_dir)\n        else:\n            print(f\"WARNING: Class '{class_name}' for GAN is not in TARGET_CLASSES. Skipping.\")\n\n\n# --- Model Registry ---\nMODELS = {\n    'CustomCNN': (None, None),\n    'VGG16': (VGG16, tf.keras.applications.vgg16.preprocess_input),\n    'VGG19': (VGG19, tf.keras.applications.vgg19.preprocess_input),\n    'ResNet50': (ResNet50, tf.keras.applications.resnet50.preprocess_input),\n    'InceptionV3': (InceptionV3, tf.keras.applications.inception_v3.preprocess_input),\n    'Xception': (Xception, tf.keras.applications.xception.preprocess_input),\n    'MobileNetV2': (MobileNetV2, tf.keras.applications.mobilenet_v2.preprocess_input),\n    'DenseNet121': (DenseNet121, tf.keras.applications.densenet.preprocess_input),\n    'EfficientNetB0': (EfficientNetB0, tf.keras.applications.efficientnet.preprocess_input),\n    'InceptionResNetV2': (InceptionResNetV2, tf.keras.applications.inception_resnet_v2.preprocess_input),\n}\n\n\n\n# --- Main Training and Evaluation Loop ---\nif NUM_CLASSES >= 2:\n    # --- DYNAMIC CONFIGURATION BASED ON CLASS COUNT ---\n    if NUM_CLASSES == 2:\n        class_mode = 'binary'\n        loss_function = 'binary_crossentropy'\n        last_layer_activation = 'sigmoid'\n        num_output_units = 1\n    else:  # Multi-class\n        class_mode = 'categorical'\n        loss_function = 'categorical_crossentropy'\n        last_layer_activation = 'softmax'\n        num_output_units = NUM_CLASSES\n\n    print(f\"\\n--- Setting up training for {NUM_CLASSES} classes ---\")\n    print(f\"Class Mode: {class_mode}, Loss: {loss_function}, Activation: {last_layer_activation}\")\n\n    for model_name, (model_constructor, preprocess_input) in MODELS.items():\n        print(f\"\\n{'='*25} Training and Evaluating: {model_name} {'='*25}\")\n        model_save_dir = os.path.join(OUTPUT_DIR, model_name)\n        os.makedirs(model_save_dir, exist_ok=True)\n\n        # --- Data Generators ---\n        # Note: In our final script, this part is handled by the more advanced\n        # get_preprocessing_function to include all filters.\n        # This corrected snippet reflects the logic from the user's provided code.\n        if model_name == 'CustomCNN':\n            train_datagen = ImageDataGenerator(rescale=1./255)\n            val_test_datagen = ImageDataGenerator(rescale=1./255)\n        else:\n            train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n            val_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n        train_generator = train_datagen.flow_from_directory(\n            train_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n            batch_size=BATCH_SIZE, class_mode=class_mode\n        )\n        validation_generator = val_test_datagen.flow_from_directory(\n            val_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n            batch_size=BATCH_SIZE, class_mode=class_mode\n        )\n        test_generator = val_test_datagen.flow_from_directory(\n            test_dir, target_size=(IMG_HEIGHT, IMG_WIDTH),\n            batch_size=BATCH_SIZE, class_mode=class_mode, shuffle=False\n        )\n\n        # --- Model Building ---\n        if model_name == 'CustomCNN':\n            model = create_custom_cnn((IMG_HEIGHT, IMG_WIDTH, 3), num_output_units, last_layer_activation)\n        else:\n            base_model = model_constructor(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n            base_model.trainable = False\n            x = GlobalAveragePooling2D(name='feature_extractor_layer')(base_model.output)\n            x = Dense(128, activation='relu')(x)\n            x = Dropout(0.5)(x)\n            predictions = Dense(num_output_units, activation=last_layer_activation)(x)\n            model = Model(inputs=base_model.input, outputs=predictions)\n\n        # --- Model Training ---\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=loss_function, metrics=['accuracy', Precision(name='precision')])\n        best_model_path = os.path.join(model_save_dir, f'{model_name}_best.keras')\n        callbacks = [\n            EarlyStopping(monitor='val_accuracy', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True),\n            ModelCheckpoint(filepath=best_model_path, save_best_only=True, monitor='val_accuracy')\n        ]\n        history = model.fit(train_generator, epochs=EPOCHS, validation_data=validation_generator, callbacks=callbacks)\n\n        # --- Evaluation & Visualization ---\n        print(f\"\\n--- Loading best model from '{best_model_path}' for evaluation ---\")\n        model = tf.keras.models.load_model(best_model_path)\n        plot_training_history(history, model_name, model_save_dir)\n\n        y_pred_prob = model.predict(test_generator)\n        y_true = test_generator.classes\n        class_names = list(test_generator.class_indices.keys())\n\n        if class_mode == 'binary':\n            y_pred = (y_pred_prob.flatten() > 0.5).astype(int)\n        else: # categorical\n            y_pred = np.argmax(y_pred_prob, axis=1)\n\n        print(f'\\nClassification Report for {model_name}:\\n')\n        print(classification_report(y_true, y_pred, target_names=class_names, digits=2))\n\n        mcc = matthews_corrcoef(y_true, y_pred)\n        print(f\"Overall Matthew's Correlation Coefficient (MCC): {mcc:.2f}\\n\")\n\n        cm = confusion_matrix(y_true, y_pred)\n        report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n\n        metrics_data = []\n        for i, class_name in enumerate(class_names):\n            TP = cm[i, i]\n            FP = cm[:, i].sum() - TP\n            FN = cm[i, :].sum() - TP\n            TN = cm.sum() - (TP + FP + FN)\n            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n            \n            metrics_data.append({\n                \"Class\": class_name,\n                \"Precision\": report_dict[class_name]['precision'],\n                \"Recall (Sensitivity)\": report_dict[class_name]['recall'],\n                \"F1-Score\": report_dict[class_name]['f1-score'],\n                \"Specificity\": specificity\n            })\n\n        metrics_df = pd.DataFrame(metrics_data)\n        print(\"Detailed Per-Class Metrics Summary:\")\n        print(metrics_df.to_string(index=False, float_format=\"%.2f\"))\n\n        plot_confusion_matrix(y_true, y_pred, class_names, model_name, model_save_dir)\n\n        y_pred_for_curves = y_pred_prob.flatten() if class_mode == 'binary' else y_pred_prob\n        plot_roc_pr_curves(y_true, y_pred_for_curves, class_names, model_name, model_save_dir)\n\n        feature_extractor = Model(inputs=model.inputs, outputs=model.get_layer('feature_extractor_layer').output)\n        test_features = feature_extractor.predict(test_generator)\n        plot_projections(test_features, y_true, class_names, model_name, model_save_dir)\n\n        last_conv_layer_name = next((layer.name for layer in reversed(model.layers) if 'conv' in layer.name.lower()), None)\n        if last_conv_layer_name and model_name != 'CustomCNN':\n            print(f\"Generating Grad-CAM for {model_name} using layer: {last_conv_layer_name}\")\n            visualize_class_maps(model, last_conv_layer_name, preprocess_input, model_name, model_save_dir, test_dir)\n        else:\n            print(f\"Skipping Grad-CAM visualization for {model_name}.\")\n\n        visualize_predictions(y_true, y_pred, test_generator, class_names, model_name, model_save_dir)\n        print(f\"\\nFinished processing {model_name}. Results saved to {model_save_dir}\")\n\n    print(\"\\nAll models have been trained and evaluated.\")\n\nelse:\n    print(\"\\nSkipping model training because a valid dataset with 2 or more classes was not created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:07:42.437789Z","iopub.execute_input":"2025-08-03T17:07:42.438501Z","iopub.status.idle":"2025-08-03T17:08:31.634821Z","shell.execute_reply.started":"2025-08-03T17:07:42.438475Z","shell.execute_reply":"2025-08-03T17:08:31.633860Z"}},"outputs":[{"name":"stdout","text":"Targeting 5 specified classes: ['Basal Cell Carcinoma', 'Squamous Cell Carcinoma', 'Melanoma', 'Actinic Keratosis', 'Vascular Lesion']\nProcessing 'Basal Cell Carcinoma'...\nProcessing 'Squamous Cell Carcinoma'...\nProcessing 'Melanoma'...\nProcessing 'Actinic Keratosis'...\nProcessing 'Vascular Lesion'...\n\nData splitting and directory setup complete. Dataset created at: '/kaggle/working/skin_cancer_dataset'\n\n--- Starting GAN Augmentation for class: Basal Cell Carcinoma ---\nTraining GAN for 50 epochs...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2396341998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCLASSES_TO_AUGMENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTARGET_CLASSES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtrain_gan_and_generate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_IMAGES_TO_GENERATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"WARNING: Class '{class_name}' for GAN is not in TARGET_CLASSES. Skipping.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3725437561.py\u001b[0m in \u001b[0;36mtrain_gan_and_generate_images\u001b[0;34m(class_name, num_to_generate, base_train_dir)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - GAN Epoch {epoch + 1}/{GAN_EPOCHS} completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_file6fjwrz4c.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 }:\n\u001b[0;32m--> 227\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    228\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_36/3725437561.py\", line 71, in train_step  *\n        fake_output = discriminator(generated_images, training=True)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\", line 227, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling Sequential.call().\n    \n    \u001b[1mInput 0 of layer \"dense_1\" is incompatible with the layer: expected axis -1 of input shape to have value 401408, but received input with shape (32, 131072)\u001b[0m\n    \n    Arguments received by Sequential.call():\n      • inputs=tf.Tensor(shape=(32, 128, 128, 3), dtype=float32)\n      • training=True\n      • mask=None\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/tmp/ipykernel_36/3725437561.py\", line 71, in train_step  *\n        fake_output = discriminator(generated_images, training=True)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\", line 227, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling Sequential.call().\n    \n    \u001b[1mInput 0 of layer \"dense_1\" is incompatible with the layer: expected axis -1 of input shape to have value 401408, but received input with shape (32, 131072)\u001b[0m\n    \n    Arguments received by Sequential.call():\n      • inputs=tf.Tensor(shape=(32, 128, 128, 3), dtype=float32)\n      • training=True\n      • mask=None\n","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}